[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Robin Penfold",
    "section": "",
    "text": "Code that I often use but rarely recall\n\n\n\n\n\nHere‚Äôs the post on this site that I view the most ‚Ä¶ and one that I hope will help you. It contains the code snippets that I often use but rarely recall. \n\n\n\n\n\nJul 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nProjecting portfolio risk\n\n\n\n\n\nRisk analysis is a standard technique within quantitative investment. In this post, I‚Äôll describe how to perform it succinctly in R.\n\n\n\n\n\nMay 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWinning at Wordle\n\n\n\n\n\nWordle is a source of healthy competition in our family. So, with a long train journey ahead of me, I thought I would embrace my nerd power and gain a competitive edge over my wife! I downloaded the most common five-letter words from the internet and analysed them with the following code. This leads me to suggest ‚Äî only in the context of Wordle ‚Äî that you should STARE at the CHILD that is FUNKY. \n\n\n\n\n\nDec 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPremPredict\n\n\n\n\n\nThey think it‚Äôs all over. It is now.\n\n\n\n\n\nSep 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAsset manager evaluation\n\n\n\n\n\nBack in the day, I wrote research papers about investment practice. One example concerned how asset owners could better evaluate the investment performance of their asset managers. Basically, the idea is to view performance in the way that a Bayesian would. \n\n\n\n\n\nOct 20, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nBrexit vote analysis\n\n\n\n\n\nI wanted more insight into the recent vote and so investigated each constituency, comparing the Brexit vote with that of the winning parliamentary party from 2016 \n\n\n\n\n\nOct 12, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nHave batting averages improved over time?\n\n\n\n\n\nCricket commentators often talk of changes in batting quality through the ages. But is there any truth to this? \n\n\n\n\n\nOct 9, 2016\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/evaluation/index.html",
    "href": "posts/evaluation/index.html",
    "title": "Asset manager evaluation",
    "section": "",
    "text": "The idea is pretty simple, which I‚Äôll outline below. (I can do so, as the materials are already in the public domain.)\nIf you want the details, check out the formal paper at SSRN. For a more accessible introduction, I‚Äôd recommend the video below that was taken (and won an award) at the International Congress of Actuaries in 2014.\n\n\n\nThe gist of the idea\nIn short, a key mantra of investment advisors is that asset owners should ignore the performance of their asset manager. Whether the manager has outperformed for you or not, it is claimed, is irrelevant to how you should expect them to perform in the future. However, a key idea in statistics, called Bayesian thinking, suggests the opposite: that you should use any new information you get about something to update your thinking on it.\nIn this work, I therefore used Bayesian thinking to understand the advice that asset owners should get. It turns out that this advice makes common sense, particularly when paired with an understanding of whether the asset manager has some longer-term cyclicality in their performance.\nHere‚Äôs that video that I mentioned:"
  },
  {
    "objectID": "posts/wordle/index.html",
    "href": "posts/wordle/index.html",
    "title": "Winning at Wordle",
    "section": "",
    "text": "Understanding the data\nLet‚Äôs start by getting the ~500 most commonly-occurring five-letter words (that I downloaded as a csv file from the internet).\n\nsuppressPackageStartupMessages(library(tidyverse))\n\nwords &lt;- \n  read_csv(\n    file = \"five-letters.csv\", \n    col_names = FALSE\n    ) |&gt; \n  rename(\"word\" = X1) |&gt; \n  mutate(word = str_to_lower(word)) |&gt; \n  mutate(\n    l1 = str_sub(string = word, start = 1, end = 1),\n    l2 = str_sub(string = word, start = 2, end = 2),\n    l3 = str_sub(string = word, start = 3, end = 3),\n    l4 = str_sub(string = word, start = 4, end = 4),\n    l5 = str_sub(string = word, start = 5, end = 5)\n  )\n\nwords\n\n\n  \n\n\n\nFor reference, I‚Äôll also chart the popularity of each letter by their order in these five-letter words.\n\nwords_long &lt;- words |&gt; \n  pivot_longer(\n    cols = -word, \n    names_to = \"measure\", \n    values_to = \"values\"\n    ) |&gt; \n  mutate(\n    position = as.integer(\n      str_sub(string = measure, start = 2)\n      )\n    ) |&gt; \n  select(values, position)\n\nwords_long |&gt; \n  count(values, position) |&gt; \n  mutate(\n    position = case_match(\n      position,\n      1 ~ \"1st\",\n      2 ~ \"2nd\",\n      3 ~ \"3rd\",\n      4 ~ \"4th\",\n      5 ~ \"5th\"\n      )\n    ) |&gt; \n  ggplot(\n    aes(\n      x = values, \n      y = n, \n      fill = values %in% c(\"a\", \"e\", \"r\", \"s\", \"t\")\n      )\n    ) + \n  geom_col() + \n  scale_y_continuous(\n    limits = c(0, NA), \n    minor_breaks = NULL, \n    expand = expansion(mult = 0, add = 1)\n    ) + \n  scale_fill_manual(values = c(\"#edd9c0\", \"#63431c\")) +\n  facet_wrap(~position, nrow = 1) + \n  theme_minimal() + \n  labs(\n    title = \"Frequency of letter by word order\",\n    subtitle = \"Emphasis on the letters a, e, r, s and t\\n\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.title.position = \"plot\"\n    )\n\n\n\n\nAs you can see from the emphasis, some of these letters appear a lot more than others, and especially at the start and end of the word.\n\n\n\nFinding the best first guess\nAs Wordle tells you if your letters are in the word and in the correct position, I‚Äôll treat the latter as more important than the former. My train journey wasn‚Äôt long enough for me to delve deeper, so I assumed that it is doubly good to guess a letter in the right position than it is to guess a correct letter in any position. Given the way that my brain works, I find it easier to guess a word if I know the first and last letter, so I‚Äôll award correct guesses in these positions with 50% more kudos than those in other positions.\nIf I incorporate all these preferences ‚Äî and remove words where a letter occurs more than once ‚Äî the following words become the best options for a first guess at Wordle.\n\nmultiplier_bonus &lt;- 2\nmultiplier_edge &lt;- 1.5\n\nfrequency &lt;- words_long |&gt; \n  count(values, position) |&gt; \n  arrange(values, position) |&gt; \n  pivot_wider(\n    id_expand = TRUE, \n    names_from = position, \n    values_from = n, \n    values_fill = 0\n    ) |&gt; \n  select(values, \"f1\" = `1`, \"f2\" = `2`, \"f3\" = `3`, \"f4\" = `4`, \"f5\" = `5`) |&gt; \n  mutate(f0 = f1 + f2 + f3 + f4 + f5)\n\nresults &lt;- words |&gt; \n  left_join(frequency |&gt; select(values, f1, f0), by = c(\"l1\" = \"values\")) |&gt; \n  rename(\"a1\" = \"f0\") |&gt; \n  left_join(frequency |&gt; select(values, f2, f0), by = c(\"l2\" = \"values\")) |&gt; \n  rename(\"a2\" = \"f0\") |&gt; \n  left_join(frequency |&gt; select(values, f3, f0), by = c(\"l3\" = \"values\")) |&gt; \n  rename(\"a3\" = \"f0\") |&gt; \n  left_join(frequency |&gt; select(values, f4, f0), by = c(\"l4\" = \"values\")) |&gt; \n  rename(\"a4\" = \"f0\") |&gt; \n  left_join(frequency |&gt; select(values, f5, f0), by = c(\"l5\" = \"values\")) |&gt; \n  rename(\"a5\" = \"f0\") |&gt; \n  mutate(\n    precise = f1 + f2 + f3 + f4 + f5,\n    bonus = (multiplier_edge * f1) + f2 + f3 + f4 + (multiplier_edge * f5),\n    general = a1 + a2 + a3 + a4 + a5,\n    total = (multiplier_bonus * bonus) + general\n    ) |&gt; \n  arrange(desc(total))\n\nresults_tidy &lt;- results |&gt; \n  select(word, l1:l5) |&gt; \n  pivot_longer(cols = l1:l5, names_to = \"value_letter\", values_to = \"values\") |&gt; \n  select(word, values) |&gt; \n  distinct() |&gt; \n  summarise(duplicates = n() != 5, .by = word) |&gt; \n  filter(!duplicates) |&gt; \n  left_join(results, by = join_by(word == word)) |&gt; \n  select(word, total, bonus, general, l1:l5)\n\nresults_tidy\n\n\n  \n\n\n\nIn other words, the best first guess at Wordle is stare. That shouldn‚Äôt be a surprise, as the chart above shows many occasions where s, t, a, r and e occur in five-letter words. Even better, s is the most frequent start to a word and e is the most common end letter in these words, giving the word a lot of ‚Äòbonus‚Äô points.\n\n\n\nFinding the best subsequent guesses\nGiven that I‚Äôve already chosen stare, what is the best second guess?1\n\nattempt1 &lt;- c(\"s\", \"t\", \"a\", \"r\", \"e\")\n\nresults_tidy |&gt; \n  filter(\n    !(l1 %in% attempt1),\n    !(l2 %in% attempt1),\n    !(l3 %in% attempt1),\n    !(l4 %in% attempt1),\n    !(l5 %in% attempt1)\n    )\n\n\n  \n\n\n\nIn this case, it looks to be child. And I can use a similar approach to find the best third guess, which turns out to be funky.\n\nattempt2 &lt;- c(\"s\", \"t\", \"a\", \"r\", \"e\", \"c\", \"h\", \"i\", \"l\", \"d\")\n\nresults_tidy |&gt; \n  filter(\n    !(l1 %in% attempt2),\n    !(l2 %in% attempt2),\n    !(l3 %in% attempt2),\n    !(l4 %in% attempt2),\n    !(l5 %in% attempt2)\n  )\n\n\n  \n\n\n\n\nWhilst I could have continued with the analysis, my train journey didn‚Äôt permit it. I was therefore left with the following best guesses, expressed as the following mnemonic:\nSTARE at the CHILD that is FUNKY\nüëÄ üßíüèª üòé\nUPDATE in July 2023: It turns out that the mnemonic above is successful strategy, as it has given me a winning streak of 201 days (and counting).\n\n\n\n\n\n\nFootnotes\n\n\nThese ‚Äòbest‚Äô guesses might not be perfect, as my assumptions above and the approach in general could probably be improved, perhaps with Operational Research techniques. That said, I suspect that ‚Äòstare‚Äô and the next guesses are decent approximations to the ideal solution.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/snippets/index.html",
    "href": "posts/snippets/index.html",
    "title": "Code that I often use but rarely recall",
    "section": "",
    "text": "Specifically, these snippets enable me to:\n\nCustomise quarto output\nGenerate ggplot2 charts in the style that I use\nFormat tables in my preferred way\nAccess databases inside and outside of shiny\nSolve common niggles in package building\n\nTo demonstrate these code snippets, I‚Äôll use the tidyverse packages with data from palmerpenguins.\n\nsuppressPackageStartupMessages(library(tidyverse))\nlibrary(palmerpenguins)\n\nNote: In each case below, you can copy the code by hovering over it and selecting the clipboard icon in the top-right of the code chunk.\n\n\n1. Customise quarto output\nGiven my frequent use of Quarto notebooks, I‚Äôve gravitated to these settings that work best for me.\nHowever, you can easily tweak these settings to suit you, as each option has an accompanying ‚Äòtab auto-complete‚Äô feature.\n---\ntitle: \"Add title here\"\nsubtitle: \"Add subtitle here\"\nauthor:\n  - Robin Penfold \ndate: today\nformat: \n  html:\n    anchor-sections: true \n    code-copy: hover \n    code-fold: true \n    code-link: true \n    code-overflow: wrap \n    code-summary: \"&lt;/&gt;\" \n    code-tools: false \n    df-print: paged \n    embed-resources: true \n    float: true\n    footnotes-hover: true \n    highlight-style: pygments\n    lang: en-GB \n    linkcolor: \"#63431c\"\n    mainfont: \"Arial\"\n    table-of-contents: true \n    toc-depth: 4\n    toc-title: \" \"\n    title-block-banner: \"#edd9c0\"\n    title-block-banner-color: \"#63431c\"\n    title-block-categories: false\neditor_options:\n  chunk_output_type: inline\n---\n\n\n\n2. Generate ggplot2 charts in the style that I use\nOver time, I have coalesced towards the following small chunk of code that builds (what I consider to be) a decent-looking chart in ggplot2.\n\npenguins |&gt; \n  ggplot(\n    aes(\n      x = bill_length_mm,\n      y = body_mass_g\n      )\n    ) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\", \n    se = FALSE, \n    colour = \"#63431c\"\n    ) +\n  labs(\n    title = \"Penguins with longer bills tend to be heavier\\n\",\n    subtitle = \"Body mass (g)\",\n    x = \"\\nBill length (mm)\",\n    y = NULL\n    ) +\n  scale_y_continuous(labels = scales::label_comma()) +\n  theme_minimal() +\n  theme(\n    plot.title.position = \"plot\",\n    plot.title = element_text(size = 14, colour = \"#63431c\"),\n    axis.title.x = element_text(hjust = 1)\n    )\n\n\n\n\n\n\n\n3. Format tables in my preferred way\nI occasionally print tables using the out-of-the-box settings (admittedly tweaked by using the df-print: paged option in part 1 above). This generates a table as follows.\n\npenguins\n\n\n  \n\n\n\nOtherwise, I usually use reactable with the following tweaks.\n\nlibrary(reactable)\n\npenguins |&gt; \n  select(species, island, bill_length_mm, bill_depth_mm, body_mass_g) |&gt; \n  reactable(\n    filterable = TRUE, \n    highlight = TRUE, \n    borderless = TRUE, \n    defaultPageSize = 5, \n    columns = list(\n      species = colDef(name = \"Species\", minWidth = 90, sticky = \"left\"),\n      island = colDef(name = \"Island\", minWidth = 90, sticky = \"left\"),\n      bill_length_mm = colDef(name = \"Bill length\", sticky = \"right\", filterable = FALSE, format = colFormat(separators = TRUE, digits = 1)),\n      bill_depth_mm = colDef(name = \"Bill depth\", sticky = \"right\", filterable = FALSE, format = colFormat(separators = TRUE, digits = 1)),\n      body_mass_g = colDef(name = \"Body mass\", sticky = \"right\", filterable = FALSE, format = colFormat(separators = TRUE, digits = 0))\n      )\n    )\n\n\n\n\n\nFor completeness, I occasionally also use DT (i.e.¬†the datatable package).\nlibrary(DT)\n\npenguins |&gt; \n  select(\"Species\" = species, \"Island\" = island, \"Bill length\" = bill_length_mm, \"Bill depth\" = bill_depth_mm, \"Body mass\" = body_mass_g) |&gt; \n  datatable(\n    rownames = FALSE, \n    width = \"100%\",\n    options=list(\n      dom = 'tip',\n      pageLength = 5\n      )\n    ) |&gt; \n  formatRound(\n    columns = 3:4,\n    digits = 1\n    ) |&gt; \n  formatRound(\n    columns = 5,\n    digits = 0\n    )\n\n\n\n\n\n\n\n4. Access databases inside and outside of shiny\nI use the wonderful DBI and dbplyr all the time, not least for exploratory analysis.\n(Note that whilst I typically don‚Äôt use SQLite, I will do so here, as it plays better with my website architecture.)\n\nlibrary(RSQLite)\n\ncon &lt;- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"instruments\", dplyr::band_instruments)\ndbWriteTable(con, \"members\", dplyr::band_members)\n\ndbListTables(con)\n\n[1] \"instruments\" \"members\"    \n\n\nIn this example, we create an object (con) for connecting to the SQLite database, where we add two tiny tables, called instruments and members. We can then explore these tables.\n\ntbl(src = con, \"instruments\")\n\n\n  \n\n\n\n\ntbl(src = con, \"members\")\n\n\n  \n\n\n\nEven better, we can explore the tables when they are combined and tidied. (Whilst the code appears to return all the data, that‚Äôs only because our tables are uncommonly small.)\n\ntbl(src = con, \"instruments\") |&gt; \n  left_join(\n    tbl(src = con, \"members\"), \n    by = \"name\"\n  ) |&gt; \n  filter(band == \"Beatles\")\n\n\n  \n\n\n\nOnce you have what you need, assign a name to the code and append it with |&gt; collect().\n\nWhilst this functionality is great outside of shiny, it is often more valuable within it. (After all, these apps can be a really safe and simple way for users to access a corporate database.)\nTo do so, some other tweaks are required within shiny‚Äôs server functionality, as illustrated below.\n\ndata_chosen &lt;- shiny::reactive({\n  shiny::req(input$dataset)\n  main_data |&gt; \n    dplyr::filter(name_dataset == input$dataset) |&gt; \n    dplyr::mutate(id = as.integer(id))\n  })\n\ndata_chosen_id &lt;- shiny::reactive(\n  quote({data_chosen()$id}),\n  quoted = TRUE\n  )\n\ndata_calculated &lt;- shiny::reactive({\n  arbitrary_function(\n    con,\n    arbitrary_argument = data_chosen_id()\n    )\n  })\n\n\n\n\n5. Solve common niggles in package building\nWhen I‚Äôm building packages, I often get dinged with notes or warnings about two common package niggles.\nThe first of these is non-ASCII characters. With apologies for the person who showed me, and who I now can‚Äôt recall, you can find these characters by:\n\nClicking CTRL + F in RStudio\nSelecting the Regex tick-box\nEntering: [\\u0080-\\uFFFF] or [^\\x00-\\x7F] as the search term\n\nFrom there, you can use stringi::stri_escape_unicode('@') to get the Unicode equivalent for @. (Note that you also might need to remove the initial ‚Äò\\‚Äô on Windows.)\n\nThe second niggle of package building concerns variable binding. It occurs during the package check and creates a note along the following lines.\nno visible binding for global variable\n    ‚ÄòABC‚Äô\nIn this case, we need to do something of the form below (from my simple package p0bservations that you can find here).\n\n#' @title Calculate income net of UK tax and National Insurance\n#'\n#' @description This function ...\n#' @param income_taxable The taxable income level ...\n#' @param tax_year_end The calendar year in which the tax year ends ...\n#' @export\n#' @examples\n#' \\dontrun{\n#' calc_income_net(income_taxable = 38000, tax_year_end = 2022L)\n#' }\n#' \n#' @importFrom rlang .data\n\nOnce we have added the line of #' @importFrom rlang .data, we can call the variables as follows (i.e.¬†as before, but preceded by .data$).\n\nyear_tax_end_options &lt;- p0bservations::tax_parameters |&gt; \n  dplyr::distinct(.data$year_tax_end) |&gt; \n  dplyr::pull(.data$year_tax_end)\n\n\nOnce again, I hope that this aide-m√©moire helps you as well as me!"
  },
  {
    "objectID": "posts/brexit/index.html",
    "href": "posts/brexit/index.html",
    "title": "Brexit vote analysis",
    "section": "",
    "text": "You can scroll down for an interactive chart of each constituency, but the following chart shows the main detail.\n\n\n&lt;/&gt;\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(ggiraph)\nlibrary(glue)\nlibrary(ggplot2)\nlibrary(ggtext)\nlibrary(parlitools)\nlibrary(scales)\n\nmap_details &lt;- west_hex_map\nsf::st_crs(map_details) = 4326\n\ndata_brexit &lt;- leave_votes_west |&gt;  \n  rename(`Leave Vote` = figure_to_use) |&gt; \n  mutate(\n    `Party of MP` = as_factor(party_2016), \n    `Party of MP` = \n      recode(\n        `Party of MP`,\n        `Scottish National Party` = \"SNP\", \n        `Liberal Democrat` = \"LibDem\"\n        ),\n    `Party of MP` = fct_lump(\n      `Party of MP`,\n      n = 5, \n      other_level = \"Other\"\n      )\n    ) |&gt; \n  left_join(\n    map_details,\n    by = c(\"ons_const_id\" = \"gss_code\")\n    ) |&gt; \n  mutate(\n    constituency = if_else(\n      is.na(constituency_name.x), \n      constituency_name.y, \n      constituency_name.x\n      ),\n    vote_status = case_when(\n      (`Leave Vote` &gt;= 0) & (`Leave Vote` &lt;= 0.4) ~ \"Remain\",\n      (`Leave Vote` &gt; 0.4) & (`Leave Vote` &lt;= 0.6) ~ \"Close Call\",\n      (`Leave Vote` &gt; 0.6) & (`Leave Vote` &lt;= 1) ~ \"Leave\",\n      TRUE ~ \"Error\"\n      ),\n    vote_status = as_factor(vote_status),\n    vote_status = fct_relevel(vote_status, sort)\n    ) |&gt; \n  select(constituency, `Leave Vote`, `Party of MP`, ons_const_id, vote_status, geometry)\n\nhtml_text &lt;- glue(\"&lt;span&gt;Showing constituencies with &lt;span style='color:#0087DC;'&gt;Conservative&lt;/span&gt;, &lt;span style='color:#DC241F;'&gt;Labour&lt;/span&gt;, &lt;span style='color:#FFFF00;'&gt;SNP&lt;/span&gt;, &lt;span style='color:#FDBB30;'&gt;LibDem&lt;/span&gt; and &lt;span style='color:#AFAFAFAF;'&gt;Other&lt;/span&gt; MPs&lt;/span&gt;&lt;br/&gt;\")\n\ndata_brexit |&gt; \n  ggplot() + \n  geom_sf(\n    aes(\n      group = constituency,\n      geometry = geometry,\n      fill = `Party of MP`\n      ),\n    size = 0.1\n    ) + \n  scale_fill_manual(\n    values = c(\n      Conservative = \"#0087DC\",\n      Labour = \"#DC241F\",\n      SNP = \"#FFFF00\",\n      LibDem = \"#FDBB30\",\n      Other = \"grey80\"\n      )\n    ) + \n  facet_grid(~vote_status, margins = TRUE, drop = TRUE) +\n  theme_void() + \n  labs(\n    title = \"Leave voters seem to have a bigger split in party allegiance\",\n    subtitle = html_text\n  ) + \n  theme(\n    plot.title.position = \"plot\",\n    plot.subtitle = element_markdown(),\n    legend.position = \"none\",\n    plot.background = element_rect(\n      fill = \"grey90\", \n      colour = \"grey90\", \n      linewidth = 1\n      ),\n    plot.margin = margin(0.5, 0.5, 0.5, 0.5, \"cm\")\n    )\n\n\n\n\n\n\n\n\n\nBasically, outside of Scotland, there seems to be a bigger split in Leaver constituencies than in their Remainer counterparts.\nTo get the details for each constituency, hover over the relevant spot in the chart below.\n\n\n&lt;/&gt;\ngg &lt;- data_brexit |&gt; \n  ggplot() + \n  geom_sf_interactive(\n    aes(\n      group = constituency,\n      geometry = geometry,\n      fill = `Leave Vote`,\n      colour = `Party of MP`,\n      tooltip = paste0(\n        constituency, \n        \"\\n MP: \",\n        `Party of MP`,\n        \"\\n Leave vote: \",\n        round(`Leave Vote` * 100, 0),\n        \"%\"\n        )\n    ),\n    size = 0.1\n  ) + \n  scale_fill_gradient_interactive(\n    low = \"white\", \n    high = \"#63666a\",\n    labels = percent_format(accuracy = 1)\n    ) +\n  scale_colour_manual_interactive(\n    values = c(\n      Conservative = \"#0087DC\",\n      Labour = \"#DC241F\",\n      SNP = \"#FFFF00\",\n      LibDem = \"#FDBB30\",\n      Other = \"grey80\"\n      )\n    ) + \n  theme_void() +\n  labs(\n    title = \"Leave vote and winning party, by constituency\", \n    subtitle = \"Hover over a point for the details\"\n  ) + \n  theme(\n    legend.position = \"right\",\n    legend.text = element_text(size = 6, hjust = 0),\n    legend.title = element_text(size = 8)\n    )\n\ngirafe(ggobj = gg)"
  },
  {
    "objectID": "posts/portfolioRisk/index.html",
    "href": "posts/portfolioRisk/index.html",
    "title": "Projecting portfolio risk",
    "section": "",
    "text": "To do so, I‚Äôll deliberately pick a minimal example and assume that our portfolio has the thirty-stock Dow-Jones Industrial Average (DJIA) as its index.\nI‚Äôll show the results in a couple of paragraphs. Before that, let‚Äôs consider a little of the theory behind this analysis. Specifically, that the active variance of a portfolio relative to its index is: \\[\\omega^2 = X^T V X\\]\n, where:\n\nX is a vector of ‚Äòexcess weights‚Äô for the thirty stocks (i.e.¬†portfolio weight less index weight)\nV is a square matrix of the covariances between the returns of the thirty stocks\n\nGiven this equation, I see that I need to begin the analysis by downloading the holdings of the index and the pricing data for the constituent stocks.\n\nlibrary(reactable)\nlibrary(tidyquant)\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(waldo)\n\n# weights_index &lt;- tq_index(\"DOW\")\n# write_rds(x = weights_index, file = \"weights_index.rds\")\n\n# stock_prices  &lt;- tq_get(\n#   x = weights_index |&gt; pull(symbol), \n#   get = \"stock.prices\", \n#   from = \"2022-01-01\")\n# write_rds(x = stock_prices, file = \"stock_prices.rds\", compress = \"xz\")\n\n\n\nStock holdings\nWith this data, I can see that the DJIA has the following stocks. For the sake of simplicity, I assume that these thirty stocks are weighted equally in our portfolio.\n\ndata_weights &lt;- read_rds(\"weights_index.rds\") |&gt; \n  mutate(\n    wt_port = 1/30,\n    wt_excess = wt_port - weight) |&gt; \n  arrange(symbol)\n\ndata_weights |&gt; \n  select(company, symbol, `excess weight` = wt_excess)\n\n\n  \n\n\n\n\n\n\nReturn covariances\nBefore I can project the risk in this portfolio, though, I also need to find the covariances between the returns of the stocks in question.1\nFor the sake of brevity, here are the first four rows and columns of this covariance matrix.\n\nreturn_covariances &lt;- read_rds(file = \"stock_prices.rds\") |&gt;\n  as_tsibble(index = date, key = symbol) |&gt; \n  mutate(yyyymm = tsibble::yearmonth(date)) |&gt; \n  slice_max(order_by = date, by = yyyymm) |&gt; \n  as_tsibble(index = date, key = symbol) |&gt; \n  mutate(return = (adjusted/lag(adjusted)) - 1) |&gt; \n  pivot_wider(id_cols = date, names_from = symbol, values_from = return) |&gt; \n  column_to_rownames(var = \"date\") |&gt; \n  cov(use = \"pairwise.complete.obs\")\n\nas_tibble(return_covariances[1:4, 1:4])\n\n\n  \n\n\n\n\n\n\nProjecting risk\nWe now have the two ingredients that we need for risk analysis: excess weights and covariances of stock returns. As a final test, let‚Äôs confirm that the order of the tickers in our vector matches that for our matrix.\n\ncompare(\n  pull(data_weights, symbol), \n  colnames(return_covariances)\n  )\n\n‚úî No differences\n\n\nWith that confirmed, I can use the formula above to project the active variance of the portfolio.\n\n(\n  variance_active_monthly &lt;- t(data_weights$wt_excess) %*% return_covariances %*% data_weights$wt_excess\n  )\n\n           [,1]\n[1,] 0.01163479\n\n\nBecause I am considering covariances with a monthly periodicity, it will help to annualise the result, so that it can be compared with corresponding values from other portfolios. To do so, I multiply the active variance by 12.2 Finally, I square-root the result to obtain \\(\\omega\\), the annualised level of projected active risk:\n\n(\n  risk_active_annual &lt;- as.numeric(\n    sqrt(variance_active_monthly * 12)\n    )\n  )\n\n[1] 0.3736543\n\n\nThis annualised ‚Äòtracking-error‚Äô of 37.4% is very high for a US equity portfolio, but is not surprising given my arbitrary choices around the portfolio and index.\n\n\n\nDecomposing risk\nThe question then becomes: ‚ÄúWhat portfolio positions contribute most to this active risk?‚Äù\nTo answer this question, we can tweak the equation above. Calculating \\(X^T V\\) generates a vector of length thirty, with each element showing a form of ‚Äòunit marginal impact‚Äô on active portfolio variance for a given stock. If we then multiply this value for a given stock by its excess weight, we obtain its contribution to monthly active portfolio variance.3\n\nvariance_active_monthly_cont &lt;- return_covariances %*% data_weights$wt_excess |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(var = \"symbol\") |&gt; \n  inner_join(data_weights, by = \"symbol\") |&gt; \n  mutate(AV_cont = wt_excess * V1) |&gt; \n  select(symbol, company, sector, wt_excess, AV_cont)\n\nvariance_active_monthly_cont |&gt; \n  select(symbol, sector, `AV contribution` = AV_cont)\n\n\n  \n\n\n\n\nTo decompose this active variance by sector, we group stocks by sector and sum their stock-level contributions.\n\nvariance_active_monthly_cont |&gt; \n  summarise(AV_total = sum(AV_cont), .by = sector) |&gt; \n  mutate(\n    sector = as_factor(sector),\n    sector = fct_reorder(sector, AV_total)) |&gt; \n  ggplot(\n    aes(\n      x = AV_total,\n      y = sector,\n      fill = AV_total &lt; 0)) + \n  labs(\n    title = \"Contribution to active variance by sector\\n\",\n    x = NULL,\n    y = NULL) +\n  geom_col() + \n  theme_minimal() + \n  theme(\n    plot.title.position = \"plot\",\n    legend.position = \"none\")\n\n\n\n\n\n\n\nConclusions\nSo, what does this analysis tell us? To me, there are two main points:\n\nThe portfolio‚Äôs tracking error is very high, although that is an artefact of the simplified nature of my example\nFinancials stocks are a key contributor to this active variance. This sector has high weights in the index but lower weights in the portfolio. However, the portfolio‚Äôs positions in Energy and Consumer Staples stocks generates a diversification benefit that reduces overall portfolio risk.\n\nAlthough these results might be useful, it is worth recognising that I have limited this risk analysis to only a standard covariance matrix, decomposed by sector. In reality, the returns data and covariance matrix could be more nuanced and decomposition could extend beyond sectors. For example, we could decompose risk by ESG scores, diversity criteria, valuation ratios or anything else that seems relevant and that can be measured.\n\n\n\n\n\n\nFootnotes\n\n\nFor the sake of simplicity, I only consider split-adjusted price returns rather than the equivalent total returns in this example‚Ü©Ô∏é\nThis might be a heroic assumption, as it implies an independence between the risks experienced in different months‚Ü©Ô∏é\nNote that the sum of these values will add to the total active variance, as this calculation is essentially vector multiplication.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/batting/index.html",
    "href": "posts/batting/index.html",
    "title": "Have batting averages improved over time?",
    "section": "",
    "text": "Batting averages seem to have changed over time, but not in the direction that you might expect.\nTypical averages have worsened over time, but only for top-order batsmen. Of course, that doesn‚Äôt necessarily mean that batting skill levels have declined, as other factors might also be involved.\nI‚Äôll now explain how I arrived at this conclusion and the assumptions that I made along the way. The starting point, as ever, was data. Specifically, the wonderful data from the espnCricinfo stats engine.\n\nlibrary(rvest)\nlibrary(tidyverse)\n\nscrape_stats &lt;- function(value_count){\n  value_url &lt;- paste0(\n    \"https://stats.espncricinfo.com/ci/engine/stats/index.html?batting_positionmax1=\",\n    value_count,\n    \";batting_positionmin1=\",\n    value_count,\n    \";batting_positionval1=batting_position;class=1;filter=advanced;opposition=1;opposition=2;opposition=3;opposition=4;opposition=5;opposition=6;opposition=7;opposition=8;orderby=runs;qualmin1=20;qualval1=innings;size=200;template=results;type=batting\"\n    )\n  \n  read_html(value_url) |&gt; \n    rvest::html_table() |&gt; \n    _[[3]]\n}\n\ndata_scraped &lt;- map(\n  .x = 1:11, \n  .f = ~scrape_stats(value_count = .x)\n  ) |&gt; \n  list_rbind(names_to = \"order_batting\")\n\nwrite_rds(x = data_scraped, file = \"data_scraped.rds\")\n\nThat said, I only consider:\n\nPlayers with twenty innings or more\nScores for and against the ‚Äòtop sides‚Äô (i.e.¬†Australia, England, India, New Zealand, Pakistan, South Africa, Sri Lanka and the West Indies) at some point\n\nAfter a little more tidying, we get the following data on about 1,000 batsmen. (In the resulting table, you can type into the text boxes below a column heading to filter the column by those values or click a column heading to order the table accordingly.)\n\nsuppressPackageStartupMessages(library(tidyverse))\nlibrary(gganimate)\nlibrary(glue)\nlibrary(magick)\nlibrary(reactable)\n\ndata_batsmen &lt;- read_rds(\"data_scraped.rds\") |&gt; \n  mutate(\n    Name = word(Player, start = 1L, end = -2L),\n    fullCountry = word(Player, -1),\n    Country = str_sub(fullCountry, 2,-2),\n    Start = as.integer(str_sub(Span, 1, 4)),\n    Decade = 10*trunc(Start/10),\n    Name = str_replace_all(Name, \"'\", \" \") \n    ) |&gt; \n  filter(!(Country %in% c(\"BAN\", \"ZIM\"))) |&gt; \n  select(Name, Country, Decade, \"Average\" = Ave, \"Innings\" = Inns, \"Order\" = order_batting) |&gt; \n  arrange(Order, desc(Average))\n\ndata_batsmen |&gt; \n  reactable(\n    filterable = TRUE, \n    highlight = TRUE, \n    borderless = TRUE, \n    defaultPageSize = 10, \n    columns = list(\n      Name = colDef(minWidth = 200, filterable = TRUE, sticky = \"left\"),\n      Country = colDef(filterable = TRUE, sticky = \"left\"),\n      Decade = colDef(filterable = TRUE, sticky = \"right\"),\n      Average = colDef(filterable = FALSE, sticky = \"right\", format = colFormat(separators = TRUE, digits = 2)),\n      Innings = colDef(filterable = FALSE, sticky = \"right\", format = colFormat(separators = TRUE, digits = 0)),\n      Order = colDef(filterable = TRUE, sticky = \"right\", format = colFormat(separators = TRUE, digits = 0))\n      )\n    )\n\n\n\n\n\n\nWe can then analyse the data by batting order, using the splendid gganimate.\n\ndata_batsmen |&gt; \n  ggplot(\n    aes(\n      x = Decade,\n      y = Average,\n      color = Country, \n      size = Innings\n      )\n    ) +\n  geom_point(alpha = 1) +\n  labs(\n    x = \"First decade of the batsman's career\",\n    y = \"\"\n    ) + \n  theme_minimal() +\n  ggtitle(\n    'Players who have ever batted at {closest_state} in the order',\n    subtitle = 'Average when batting at that position'\n    ) + \n  transition_states(\n    states = Order,\n    transition_length = 2,\n    state_length = 1\n    ) + \n  ease_aes('cubic-in-out') \n\n\n\n\n\n\n\n\n\nAlthough that looks cool, I find it hard to detect patterns in animations, and so switch to a static plot.\n\ndata_batsmen |&gt; \n  ggplot(\n    aes(\n      x = Order,\n      y = Average,\n      colour = Decade\n      )\n    ) +\n  geom_jitter() +\n  scale_x_continuous(labels = c(1, 3, 5, 7, 9, 11), breaks = c(1, 3, 5, 7, 9, 11)) +\n  scale_y_continuous(labels = c(0, 50, 100), breaks = c(0, 50, 100)) +\n  facet_wrap(~Decade) +\n  theme_minimal() +\n  labs(\n    title = 'Data is limited before the 1950s\\n',\n    subtitle = 'Batting average, when batting at that position, for a career that started in the given decade',\n    x = \"\\nBatting order\",\n    y = NULL\n    ) + \n  theme(\n    plot.title.position = \"plot\",\n    legend.position = \"none\",\n    axis.title.x = element_text(hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\nGiven the paucity of data before the 1950s and in this decade, it makes sense to drop these decades from our analysis. If we do so and also add best-fit lines to the decade-by-decade charts above, we get the following.\n\ndata_batsmen |&gt; \n  filter(\n    between(Decade, 1950, 2010)\n  ) |&gt;\n  mutate(Decade = as_factor(Decade)) |&gt; \n  ggplot(\n    aes(\n      x = Order,\n      y = Average,\n      colour = Decade\n      )\n    ) +\n  geom_jitter() + \n  geom_smooth(se = FALSE) + \n  scale_colour_brewer(palette = \"Blues\", direction = -1) +\n  scale_x_continuous(labels = c(1, 3, 5, 7, 9, 11), breaks = c(1, 3, 5, 7, 9, 11)) +\n  scale_y_continuous(labels = c(0, 50, 100), breaks = c(0, 50, 100)) +\n  facet_wrap(~Decade) +\n  theme_minimal() +\n  labs(\n    title = 'Typical averages by batting order follow a similar shape over the decades\\n',\n    subtitle = 'Batting average, when batting at that position, for a career that started in the given decade',\n    x = \"\\nBatting order\",\n    y = NULL\n    ) + \n  theme(\n    plot.title.position = \"plot\",\n    legend.position = \"none\",\n    axis.title.x = element_text(hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\nTo get a better sense of the trends, let‚Äôs now just consider these best-fit lines as one chart, where darker lines represent earlier starting decades of a career.\n\ndata_batsmen |&gt; \n  filter(\n    between(Decade, 1950, 2010)\n  ) |&gt; \n  mutate(Decade = as_factor(Decade)) |&gt; \n  ggplot(\n    aes(\n      x = Order,\n      y = Average,\n      colour = Decade,\n      group = Decade\n      )\n    ) +\n  geom_smooth(se = FALSE) + \n  scale_colour_brewer(palette = \"Blues\", direction = -1) +\n  scale_x_continuous(labels = 1:11, breaks = 1:11, minor_breaks = NULL) +\n  scale_y_continuous(minor_breaks = NULL, limits = c(0, NA), labels = 20 * 0:2, breaks = 20 * 0:2) +\n  theme_minimal() +\n  labs(\n    title = 'Higher-order averages have typically declined since the 1950s\\n',\n    subtitle = 'Typical batting average, when batting at that position, for a career that started in the given decade',\n    colour = \"Decade start\",\n    x = \"\\nBatting order\",\n    y = NULL\n    ) + \n  theme(\n    plot.title.position = \"plot\",\n    axis.title.x = element_text(hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\nSo, what does this chart show us?\nTo me, it shows that typical latter-order averages haven‚Äôt changed that much over the decades, whilst there‚Äôs a lot more variation by decade for the typical averages of higher-order batsmen. Furthermore, the lines with higher typical top-order averages all seem to be darker (and therefore earlier).\nTo get more specific, let‚Äôs split this top-order data into two eras: the 1950s to the 1970s; and the 1980s and beyond. I can then compare the typical top-order batting averages in these eras.\n\ndata_test &lt;- data_batsmen |&gt; \n  filter(\n    between(Decade, 1950, 2010),\n    Order &lt;= 5\n  ) |&gt; \n  mutate(is_1950s_to_1970s = Decade &lt;= 1970) |&gt; \n  select(Average, is_1950s_to_1970s) \n\nt.test(\n  data = data_test, \n  Average ~ is_1950s_to_1970s, \n  alternative = \"less\"\n  )\n\n\n    Welch Two Sample t-test\n\ndata:  Average by is_1950s_to_1970s\nt = -2.0788, df = 348.5, p-value = 0.01918\nalternative hypothesis: true difference in means between group FALSE and group TRUE is less than 0\n95 percent confidence interval:\n       -Inf -0.4100609\nsample estimates:\nmean in group FALSE  mean in group TRUE \n           39.55741            41.54190 \n\n\n\nOK, so that‚Äôs a lot of detail! But what does it show?\nEssentially, it says that the reduction in typical top-order batting averages since the 1950s to 1980s is statistically significant. (That is, beyond what we would usually expect from chance alone.)\nOf course, this reduction might not be solely due to lower inherent skill levels relative to the prevailing bowlers. Two possible reasons are that:\n\nThe game is now different to before, with DRS, helmets and masses of analysis\nSurvivor bias might be a factor. After all, we are only considering players with twenty or more innings, for and against the ‚Äòtop sides‚Äô; a point also complicated by the fact that there are now more test matches per year than in the past\n\nAs is often the way with cricket, so much is in the interpretation and debate. And that‚Äôs just another reason to love it!\n\n\n\n[Updated on March 25, 2023]"
  },
  {
    "objectID": "posts/prempredict/index.html",
    "href": "posts/prempredict/index.html",
    "title": "PremPredict",
    "section": "",
    "text": "After a good run of thirteen years, now feels like the right time to end our PremPredict competition.\nThanks to all of you who have shared in the fun over this time. I‚Äôm pleased to say that this included Les Penfold, Mike Finnis and Roger Gathercole.\nAs you‚Äôll see below, though, this post and the Wall of Fame will remain here, always showing that Peter Finnis was left holding the crown!\n(Oh yeah ‚Äì and always showing how some of the biggest football nerds remained winless.)\n\n\n\nPremPredict champions\n\n2007/08 ‚Äì Robert Wye\n2008/09 ‚Äì Les Penfold\n2009/10 ‚Äì Beth Penfold\n2010/11 ‚Äì George Quin and Miranda Stride\n2011/12 ‚Äì Hannah Finnis\n2012/13 ‚Äì Michael Cheeseman\n2013/14 ‚Äì Danny Russell\n2014/15 ‚Äì Mathew Saunders\n2015/16 ‚Äì Roger Gathercole\n2016/17 ‚Äì Luke Finnis\n2017/18 ‚Äì George Quin\n2018/19 ‚Äì Roger Gathercole\n2019/20 ‚Äì Peter Finnis\n\n\nThanks again for all the fun!"
  }
]