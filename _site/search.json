[
  {
    "objectID": "posts/brexit/index.html",
    "href": "posts/brexit/index.html",
    "title": "Brexit vote analysis",
    "section": "",
    "text": "You can scroll down for an interactive chart of each constituency, but the following chart shows the main detail.\n\n\n</>\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(ggiraph)\nlibrary(glue)\nlibrary(ggplot2)\nlibrary(ggtext)\nlibrary(parlitools)\nlibrary(scales)\n\nmap_details <- west_hex_map\nsf::st_crs(map_details) = 4326\n\ndata_brexit <- leave_votes_west |>  \n  rename(`Leave Vote` = figure_to_use) |> \n  mutate(\n    `Party of MP` = as_factor(party_2016), \n    `Party of MP` = \n      recode(\n        `Party of MP`,\n        `Scottish National Party` = \"SNP\", \n        `Liberal Democrat` = \"LibDem\"\n        ),\n    `Party of MP` = fct_lump(\n      `Party of MP`,\n      n = 5, \n      other_level = \"Other\"\n      )\n    ) |> \n  left_join(\n    map_details,\n    by = c(\"ons_const_id\" = \"gss_code\")\n    ) |> \n  mutate(\n    constituency = if_else(\n      is.na(constituency_name.x), \n      constituency_name.y, \n      constituency_name.x\n      ),\n    vote_status = case_when(\n      (`Leave Vote` >= 0) & (`Leave Vote` <= 0.4) ~ \"Remain\",\n      (`Leave Vote` > 0.4) & (`Leave Vote` <= 0.6) ~ \"Close Call\",\n      (`Leave Vote` > 0.6) & (`Leave Vote` <= 1) ~ \"Leave\",\n      TRUE ~ \"Error\"\n      ),\n    vote_status = as_factor(vote_status),\n    vote_status = fct_relevel(vote_status, sort)\n    ) |> \n  select(constituency, `Leave Vote`, `Party of MP`, ons_const_id, vote_status, geometry)\n\nhtml_text <- glue(\"<span>Showing constituencies with <span style='color:#0087DC;'>Conservative</span>, <span style='color:#DC241F;'>Labour</span>, <span style='color:#FFFF00;'>SNP</span>, <span style='color:#FDBB30;'>LibDem</span> and <span style='color:#AFAFAFAF;'>Other</span> MPs</span><br/>\")\n\ndata_brexit |> \n  ggplot() + \n  geom_sf(\n    aes(\n      group = constituency,\n      geometry = geometry,\n      fill = `Party of MP`\n      ),\n    size = 0.1\n    ) + \n  scale_fill_manual(\n    values = c(\n      Conservative = \"#0087DC\",\n      Labour = \"#DC241F\",\n      SNP = \"#FFFF00\",\n      LibDem = \"#FDBB30\",\n      Other = \"grey80\"\n      )\n    ) + \n  facet_grid(~vote_status, margins = TRUE, drop = TRUE) +\n  theme_void() + \n  labs(\n    title = \"Leave voters seem to have a bigger split in party allegiance\",\n    subtitle = html_text\n  ) + \n  theme(\n    plot.title.position = \"plot\",\n    plot.subtitle = element_markdown(),\n    legend.position = \"none\",\n    plot.background = element_rect(\n      fill = \"grey90\", \n      colour = \"grey90\", \n      linewidth = 1\n      ),\n    plot.margin = margin(0.5, 0.5, 0.5, 0.5, \"cm\")\n    )\n\n\n\n\n\nBasically, outside of Scotland, there seems to be a bigger split in Leaver constituencies than in their Remainer counterparts.\nTo get the details for each constituency, hover over the relevant spot in the chart below.\n\n\n</>\ngg <- data_brexit |> \n  ggplot() + \n  geom_sf_interactive(\n    aes(\n      group = constituency,\n      geometry = geometry,\n      fill = `Leave Vote`,\n      colour = `Party of MP`,\n      tooltip = paste0(\n        constituency, \n        \"\\n MP: \",\n        `Party of MP`,\n        \"\\n Leave vote: \",\n        round(`Leave Vote` * 100, 0),\n        \"%\"\n        )\n    ),\n    size = 0.1\n  ) + \n  scale_fill_gradient_interactive(\n    low = \"white\", \n    high = \"#63666a\",\n    labels = percent_format(accuracy = 1)\n    ) +\n  scale_colour_manual_interactive(\n    values = c(\n      Conservative = \"#0087DC\",\n      Labour = \"#DC241F\",\n      SNP = \"#FFFF00\",\n      LibDem = \"#FDBB30\",\n      Other = \"grey80\"\n      )\n    ) + \n  theme_void() +\n  labs(\n    title = \"Leave vote and winning party, by constituency\", \n    subtitle = \"Hover over a point for the details\"\n  ) + \n  theme(\n    legend.position = \"right\",\n    legend.text = element_text(size = 6, hjust = 0),\n    legend.title = element_text(size = 8)\n    )\n\ngirafe(ggobj = gg)"
  },
  {
    "objectID": "posts/portfolioRisk/index.html",
    "href": "posts/portfolioRisk/index.html",
    "title": "Projecting portfolio risk",
    "section": "",
    "text": "To do so, I’ll deliberately pick a minimal example and assume that our portfolio has the thirty-stock Dow-Jones Industrial Average (DJIA) as its index.\nI’ll show the results in a couple of paragraphs. Before that, let’s consider a little of the theory behind this analysis. Specifically, that the active variance of a portfolio relative to its index is: \\[\\omega^2 = X^T V X\\]\n, where:\n\nX is a vector of ‘excess weights’ for the thirty stocks (i.e. portfolio weight less index weight)\nV is a square matrix of the covariances between the returns of the thirty stocks\n\nGiven this equation, I see that I need to begin the analysis by downloading the holdings of the index and the pricing data for the constituent stocks.\n\n\n</>\nlibrary(reactable)\nlibrary(tidyquant)\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(waldo)\n\n# weights_index <- tq_index(\"DOW\")\n# write_rds(x = weights_index, file = \"weights_index.rds\")\n\n# stock_prices  <- tq_get(\n#   x = weights_index |> pull(symbol), \n#   get = \"stock.prices\", \n#   from = \"2022-01-01\")\n# write_rds(x = stock_prices, file = \"stock_prices.rds\", compress = \"xz\")\n\n\n\n\nStock holdings\nWith this data, I can see that the DJIA has the following stocks. For the sake of simplicity, I assume that these thirty stocks are weighted equally in our portfolio.\n\n\n</>\ndata_weights <- read_rds(\"weights_index.rds\") |> \n  mutate(\n    wt_port = 1/30,\n    wt_excess = wt_port - weight) |> \n  arrange(symbol)\n\ndata_weights |> \n  select(company, symbol, `excess weight` = wt_excess)\n\n\n\n\n  \n\n\n\n\n\n\nReturn covariances\nBefore I can project the risk in this portfolio, though, I also need to find the covariances between the returns of the stocks in question.1\nFor the sake of brevity, here are the first four rows and columns of this covariance matrix.\n\n\n</>\nreturn_covariances <- read_rds(file = \"stock_prices.rds\") |>\n  as_tsibble(index = date, key = symbol) |> \n  mutate(yyyymm = tsibble::yearmonth(date)) |> \n  slice_max(order_by = date, by = yyyymm) |> \n  as_tsibble(index = date, key = symbol) |> \n  mutate(return = (adjusted/lag(adjusted)) - 1) |> \n  pivot_wider(id_cols = date, names_from = symbol, values_from = return) |> \n  column_to_rownames(var = \"date\") |> \n  cov(use = \"pairwise.complete.obs\")\n\nas_tibble(return_covariances[1:4, 1:4])\n\n\n\n\n  \n\n\n\n\n\n\nProjecting risk\nWe now have the two ingredients that we need for risk analysis: excess weights and covariances of stock returns. As a final test, let’s confirm that the order of the tickers in our vector matches that for our matrix.\n\n\n</>\ncompare(\n  pull(data_weights, symbol), \n  colnames(return_covariances)\n  )\n\n\n✔ No differences\n\n\nWith that confirmed, I can use the formula above to project the active variance of the portfolio.\n\n\n</>\n(variance_active_monthly <- t(data_weights$wt_excess) %*% return_covariances %*% data_weights$wt_excess)\n\n\n           [,1]\n[1,] 0.01163479\n\n\nBecause I am considering covariances with a monthly periodicity, it will help to annualise the result, so that it can be compared with corresponding values from other portfolios. To do so, I multiply the active variance by 12.2 Finally, I square-root the result to obtain \\(\\omega\\), the annualised level of projected active risk:\n\n\n</>\n(risk_active_annual <- as.numeric(sqrt(variance_active_monthly * 12)))\n\n\n[1] 0.3736543\n\n\nThis annualised ‘tracking-error’ of 37.4% is very high for a US equity portfolio, but is not surprising given my arbitrary choices around the portfolio and index.\n\n\n\nDecomposing risk\nThe question then becomes: “What portfolio positions contribute most to this active risk?”\nTo answer this question, we can tweak the equation above. Calculating \\(X^T V\\) generates a vector of length thirty, with each element showing a form of ‘unit marginal impact’ on active portfolio variance for a given stock. If we then multiply this value for a given stock by its excess weight, we obtain its contribution to monthly active portfolio variance.3\n\n\n</>\nvariance_active_monthly_cont <- return_covariances %*% data_weights$wt_excess |> \n  as.data.frame() |> \n  rownames_to_column(var = \"symbol\") |> \n  inner_join(data_weights, by = \"symbol\") |> \n  mutate(AV_cont = wt_excess * V1) |> \n  select(symbol, company, sector, wt_excess, AV_cont)\n\nvariance_active_monthly_cont |> \n  select(symbol, sector, `AV contribution` = AV_cont)\n\n\n\n\n  \n\n\n\n\nTo decompose this active variance by sector, we group stocks by sector and sum their stock-level contributions.\n\n\n</>\nvariance_active_monthly_cont |> \n  summarise(AV_total = sum(AV_cont), .by = sector) |> \n  mutate(\n    sector = as_factor(sector),\n    sector = fct_reorder(sector, AV_total)) |> \n  ggplot(\n    aes(\n      x = AV_total,\n      y = sector,\n      fill = AV_total < 0)) + \n  labs(\n    title = \"Contribution to active variance by sector\\n\",\n    x = NULL,\n    y = NULL) +\n  geom_col() + \n  theme_minimal() + \n  theme(\n    plot.title.position = \"plot\",\n    legend.position = \"none\")\n\n\n\n\n\n\n\n\nConclusions\nSo, what does this analysis tell us? To me, there are two main points:\n\nThe portfolio’s tracking error is very high, although that is an artefact of the simplified nature of my example\nFinancials stocks are a key contributor to this active variance. This sector has high weights in the index but lower weights in the portfolio. However, the portfolio’s positions in Energy and Consumer Staples stocks generates a diversification benefit that reduces overall portfolio risk.\n\nAlthough these results might be useful, it is worth recognising that I have limited this risk analysis to only a standard covariance matrix, decomposed by sector. In reality, the returns data and covariance matrix could be more nuanced and decomposition could extend beyond sectors. For example, we could decompose risk by ESG scores, diversity criteria, valuation ratios or anything else that seems relevant and that can be measured.\n\n\n\n\n\n\nFootnotes\n\n\nFor the sake of simplicity, I only consider split-adjusted price returns rather than the equivalent total returns in this example↩︎\nThis might be a heroic assumption, as it implies an independence between the risks experienced in different months↩︎\nNote that the sum of these values will add to the total active variance, as this calculation is essentially vector multiplication.↩︎"
  },
  {
    "objectID": "posts/batting/index.html",
    "href": "posts/batting/index.html",
    "title": "Have batting averages improved over time?",
    "section": "",
    "text": "Batting averages seem to have changed over time, but not in the direction that you might expect.\nTypical averages have worsened over time, but only for top-order batsmen. Of course, that doesn’t necessarily mean that batting skill levels have declined, as other factors might also be involved.\nI’ll now explain how I arrived at this conclusion and the assumptions that I made along the way. The starting point, as ever, was data. Specifically, the wonderful data from the espnCricinfo stats engine.\n\nlibrary(rvest)\nlibrary(tidyverse)\n\nscrape_stats <- function(value_count){\n  value_url <- paste0(\n    \"https://stats.espncricinfo.com/ci/engine/stats/index.html?batting_positionmax1=\",\n    value_count,\n    \";batting_positionmin1=\",\n    value_count,\n    \";batting_positionval1=batting_position;class=1;filter=advanced;opposition=1;opposition=2;opposition=3;opposition=4;opposition=5;opposition=6;opposition=7;opposition=8;orderby=runs;qualmin1=20;qualval1=innings;size=200;template=results;type=batting\"\n    )\n  \n  read_html(value_url) |> \n    rvest::html_table() |> \n    _[[3]]\n}\n\ndata_scraped <- map(\n  .x = 1:11, \n  .f = ~scrape_stats(value_count = .x)\n  ) |> \n  list_rbind(names_to = \"order_batting\")\n\nwrite_rds(x = data_scraped, file = \"data_scraped.rds\")\n\nThat said, I only consider:\n\nPlayers with twenty innings or more\nScores for and against the ‘top sides’ (i.e. Australia, England, India, New Zealand, Pakistan, South Africa, Sri Lanka and the West Indies) at some point\n\nAfter a little more tidying, we get the following data on about 1,000 batsmen. (In the resulting table, you can type into the text boxes below a column heading to filter the column by those values or click a column heading to order the table accordingly.)\n\nsuppressPackageStartupMessages(library(tidyverse))\nlibrary(gganimate)\nlibrary(glue)\nlibrary(magick)\nlibrary(reactable)\n\ndata_batsmen <- read_rds(\"data_scraped.rds\") |> \n  mutate(\n    Name = word(Player, start = 1L, end = -2L),\n    fullCountry = word(Player, -1),\n    Country = str_sub(fullCountry, 2,-2),\n    Start = as.integer(str_sub(Span, 1, 4)),\n    Decade = 10*trunc(Start/10),\n    Name = str_replace_all(Name, \"'\", \" \") \n    ) |> \n  filter(!(Country %in% c(\"BAN\", \"ZIM\"))) |> \n  select(Name, Country, Decade, \"Average\" = Ave, \"Innings\" = Inns, \"Order\" = order_batting) |> \n  arrange(Order, desc(Average))\n\ndata_batsmen |> \n  reactable(\n    filterable = TRUE, \n    highlight = TRUE, \n    borderless = TRUE, \n    defaultPageSize = 10, \n    columns = list(\n      Name = colDef(minWidth = 200, filterable = TRUE, sticky = \"left\"),\n      Country = colDef(filterable = TRUE, sticky = \"left\"),\n      Decade = colDef(filterable = TRUE, sticky = \"right\"),\n      Average = colDef(filterable = FALSE, sticky = \"right\", format = colFormat(separators = TRUE, digits = 2)),\n      Innings = colDef(filterable = FALSE, sticky = \"right\", format = colFormat(separators = TRUE, digits = 0)),\n      Order = colDef(filterable = TRUE, sticky = \"right\", format = colFormat(separators = TRUE, digits = 0))\n      )\n    )\n\n\n\n\n\n\n\nWe can then analyse the data by batting order, using the splendid gganimate.\n\ndata_batsmen |> \n  ggplot(\n    aes(\n      x = Decade,\n      y = Average,\n      color = Country, \n      size = Innings\n      )\n    ) +\n  geom_point(alpha = 1) +\n  labs(\n    x = \"First decade of the batsman's career\",\n    y = \"\"\n    ) + \n  theme_minimal() +\n  ggtitle(\n    'Players who have ever batted at {closest_state} in the order',\n    subtitle = 'Average when batting at that position'\n    ) + \n  transition_states(\n    states = Order,\n    transition_length = 2,\n    state_length = 1\n    ) + \n  ease_aes('cubic-in-out') \n\n\n\n\n\nAlthough that looks cool, I find it hard to detect patterns in animations, and so switch to a static plot.\n\ndata_batsmen |> \n  ggplot(\n    aes(\n      x = Order,\n      y = Average,\n      colour = Decade\n      )\n    ) +\n  geom_jitter() +\n  scale_x_continuous(labels = c(1, 3, 5, 7, 9, 11), breaks = c(1, 3, 5, 7, 9, 11)) +\n  scale_y_continuous(labels = c(0, 50, 100), breaks = c(0, 50, 100)) +\n  facet_wrap(~Decade) +\n  theme_minimal() +\n  labs(\n    title = 'Data is limited before the 1950s\\n',\n    subtitle = 'Batting average, when batting at that position, for a career that started in the given decade',\n    x = \"\\nBatting order\",\n    y = NULL\n    ) + \n  theme(\n    plot.title.position = \"plot\",\n    legend.position = \"none\",\n    axis.title.x = element_text(hjust = 1)\n  )\n\n\n\n\n\nGiven the paucity of data before the 1950s and in this decade, it makes sense to drop these decades from our analysis. If we do so and also add best-fit lines to the decade-by-decade charts above, we get the following.\n\ndata_batsmen |> \n  filter(\n    between(Decade, 1950, 2010)\n  ) |>\n  mutate(Decade = as_factor(Decade)) |> \n  ggplot(\n    aes(\n      x = Order,\n      y = Average,\n      colour = Decade\n      )\n    ) +\n  geom_jitter() + \n  geom_smooth(se = FALSE) + \n  scale_colour_brewer(palette = \"Blues\", direction = -1) +\n  scale_x_continuous(labels = c(1, 3, 5, 7, 9, 11), breaks = c(1, 3, 5, 7, 9, 11)) +\n  scale_y_continuous(labels = c(0, 50, 100), breaks = c(0, 50, 100)) +\n  facet_wrap(~Decade) +\n  theme_minimal() +\n  labs(\n    title = 'Typical averages by batting order follow a similar shape over the decades\\n',\n    subtitle = 'Batting average, when batting at that position, for a career that started in the given decade',\n    x = \"\\nBatting order\",\n    y = NULL\n    ) + \n  theme(\n    plot.title.position = \"plot\",\n    legend.position = \"none\",\n    axis.title.x = element_text(hjust = 1)\n  )\n\n\n\n\n\nTo get a better sense of the trends, let’s now just consider these best-fit lines as one chart, where darker lines represent earlier starting decades of a career.\n\ndata_batsmen |> \n  filter(\n    between(Decade, 1950, 2010)\n  ) |> \n  mutate(Decade = as_factor(Decade)) |> \n  ggplot(\n    aes(\n      x = Order,\n      y = Average,\n      colour = Decade,\n      group = Decade\n      )\n    ) +\n  geom_smooth(se = FALSE) + \n  scale_colour_brewer(palette = \"Blues\", direction = -1) +\n  scale_x_continuous(labels = 1:11, breaks = 1:11, minor_breaks = NULL) +\n  scale_y_continuous(minor_breaks = NULL, limits = c(0, NA), labels = 20 * 0:2, breaks = 20 * 0:2) +\n  theme_minimal() +\n  labs(\n    title = 'Higher-order averages have typically declined since the 1950s\\n',\n    subtitle = 'Typical batting average, when batting at that position, for a career that started in the given decade',\n    colour = \"Decade start\",\n    x = \"\\nBatting order\",\n    y = NULL\n    ) + \n  theme(\n    plot.title.position = \"plot\",\n    axis.title.x = element_text(hjust = 1)\n  )\n\n\n\n\n\nSo, what does this chart show us?\nTo me, it shows that typical latter-order averages haven’t changed that much over the decades, whilst there’s a lot more variation by decade for the typical averages of higher-order batsmen. Furthermore, the lines with higher typical top-order averages all seem to be darker (and therefore earlier).\nTo get more specific, let’s split this top-order data into two eras: the 1950s to the 1970s; and the 1980s and beyond. I can then compare the typical top-order batting averages in these eras.\n\ndata_test <- data_batsmen |> \n  filter(\n    between(Decade, 1950, 2010),\n    Order <= 5\n  ) |> \n  mutate(is_1950s_to_1970s = Decade <= 1970) |> \n  select(Average, is_1950s_to_1970s) \n\nt.test(\n  data = data_test, \n  Average ~ is_1950s_to_1970s, \n  alternative = \"less\"\n  )\n\n\n    Welch Two Sample t-test\n\ndata:  Average by is_1950s_to_1970s\nt = -2.0788, df = 348.5, p-value = 0.01918\nalternative hypothesis: true difference in means between group FALSE and group TRUE is less than 0\n95 percent confidence interval:\n       -Inf -0.4100609\nsample estimates:\nmean in group FALSE  mean in group TRUE \n           39.55741            41.54190 \n\n\n\nOK, so that’s a lot of detail! But what does it show?\nEssentially, it says that the reduction in typical top-order batting averages since the 1950s to 1980s is statistically significant. (That is, beyond what we would usually expect from chance alone.)\nOf course, this reduction might not be solely due to lower inherent skill levels relative to the prevailing bowlers. Two possible reasons are that:\n\nThe game is now different to before, with DRS, helmets and masses of analysis\nSurvivor bias might be a factor. After all, we are only considering players with twenty or more innings, for and against the ‘top sides’; a point also complicated by the fact that there are now more test matches per year than in the past\n\nAs is often the way with cricket, so much is in the interpretation and debate. And that’s just another reason to love it!\n\n\n\n[Updated on March 25, 2023]"
  },
  {
    "objectID": "posts/evaluation/index.html",
    "href": "posts/evaluation/index.html",
    "title": "Asset manager evaluation",
    "section": "",
    "text": "The idea is pretty simple, which I’ll outline below. (I can do so, as the materials are already in the public domain.)\nIf you want the details, check out the formal paper at SSRN. For a more accessible introduction, I’d recommend the video below that was taken (and won an award) at the International Congress of Actuaries in 2014.\n\n\n\nThe gist of the idea\nIn short, a key mantra of investment advisors is that asset owners should ignore the performance of their asset manager. Whether the manager has outperformed for you or not, it is claimed, is irrelevant to how you should expect them to perform in the future. However, a key idea in statistics, called Bayesian thinking, suggests the opposite: that you should use any new information you get about something to update your thinking on it.\nIn this work, I therefore used Bayesian thinking to understand the advice that asset owners should get. It turns out that this advice makes common sense, particularly when paired with an understanding of whether the asset manager has some longer-term cyclicality in their performance.\nHere’s that video that I mentioned:"
  },
  {
    "objectID": "posts/prempredict/index.html",
    "href": "posts/prempredict/index.html",
    "title": "PremPredict",
    "section": "",
    "text": "After a good run of thirteen years, now feels like the right time to end our PremPredict competition.\nThanks to all of you who have shared in the fun over this time. I’m pleased to say that this included Les Penfold, Mike Finnis and Roger Gathercole.\nAs you’ll see below, though, this post and the Wall of Fame will remain here, always showing that Peter Finnis was left holding the crown!\n(Oh yeah – and always showing how some of the biggest football nerds remained winless.)\n\n\n\nPremPredict champions\n\n2007/08 – Robert Wye\n2008/09 – Les Penfold\n2009/10 – Beth Penfold\n2010/11 – George Quin and Miranda Stride\n2011/12 – Hannah Finnis\n2012/13 – Michael Cheeseman\n2013/14 – Danny Russell\n2014/15 – Mathew Saunders\n2015/16 – Roger Gathercole\n2016/17 – Luke Finnis\n2017/18 – George Quin\n2018/19 – Roger Gathercole\n2019/20 – Peter Finnis\n\n\nThanks again for all the fun!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Robin Penfold",
    "section": "",
    "text": "Projecting portfolio risk\n\n\n\n\n\nRisk analysis is a standard technique within quantitative investment. In this post, I’ll describe how to perform it succinctly in R.\n\n\n\n\n\n\nMay 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPremPredict\n\n\n\n\n\nThey think it’s all over. It is now.\n\n\n\n\n\n\nSep 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\nAsset manager evaluation\n\n\n\n\n\nBack in the day, I wrote research papers about investment practice. One example concerned how asset owners could better evaluate the investment performance of their asset managers. Basically, the idea is to view performance in the way that a Bayesian would.\n\n\n\n\n\n\nOct 20, 2016\n\n\n\n\n\n\n  \n\n\n\n\nBrexit vote analysis\n\n\n\n\n\nI wanted more insight into the recent vote and so investigated each constituency, comparing the Brexit vote with that of the winning parliamentary party from 2016\n\n\n\n\n\n\nOct 12, 2016\n\n\n\n\n\n\n  \n\n\n\n\nHave batting averages improved over time?\n\n\n\n\n\nCricket commentators often talk of changes in batting quality through the ages. But is there any truth to this?\n\n\n\n\n\n\nOct 9, 2016\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "At work – Quant analysis of asset managers, using data science and investment knowledge\nAs a volunteer – Helping charities to get more from their data with DataKind\nOtherwise – With my family and friends, and supporting sports teams that rarely win"
  }
]