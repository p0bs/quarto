[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Robin's Quanty Ramblings",
    "section": "",
    "text": "Making sense of English seasons\n\n\n\n\n\nOr how it only takes four days of Autumn cooling to undo five days of Spring warming. \n\n\n\n\n\nJan 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCode that I often use but rarely recall\n\n\n\n\n\nHere‚Äôs the post on this site that I view the most ‚Ä¶ and one that I hope will help you. It contains the code snippets that I often use but rarely recall. \n\n\n\n\n\nJul 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nProjecting portfolio risk\n\n\n\n\n\nRisk analysis is a standard technique within quantitative investment. In this post, I‚Äôll describe how to perform it succinctly in R.\n\n\n\n\n\nMay 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWinning at Wordle\n\n\n\n\n\nWordle is a source of healthy competition in our family. So, with a long train journey ahead of me, I thought I would embrace my nerd power and gain a competitive edge over my wife! I downloaded the most common five-letter words from the internet and analysed them with the following code. This leads me to suggest ‚Äî only in the context of Wordle ‚Äî that you should STARE at the CHILD that is FUNKY. \n\n\n\n\n\nDec 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPremPredict\n\n\n\n\n\nThey think it‚Äôs all over. It is now.\n\n\n\n\n\nSep 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAsset manager evaluation\n\n\n\n\n\nBack in the day, I wrote research papers about investment practice. One example concerned how asset owners could better evaluate the investment performance of their asset managers. Basically, the idea is to view performance in the way that a Bayesian would. \n\n\n\n\n\nOct 20, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nBrexit vote analysis\n\n\n\n\n\nI wanted more insight into the recent vote and so investigated each constituency, comparing the Brexit vote with that of the winning parliamentary party from 2016 \n\n\n\n\n\nOct 12, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nHave batting averages improved over time?\n\n\n\n\n\nCricket commentators often talk of changes in batting quality through the ages. But is there any truth to this? \n\n\n\n\n\nOct 9, 2016\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/temperature/index.html",
    "href": "posts/temperature/index.html",
    "title": "Making sense of English seasons",
    "section": "",
    "text": "Like so many Brits, I‚Äôm obsessed with the weather. I ask questions like, ‚ÄúWhen is spring coming?‚Äù and ‚ÄúHow long is summer?‚Äù\nAnnoyingly, though, I never had reasonable answers for these sorts of questions. And, for that reason, I thought I‚Äôd consult the data to get better answers.\nI turns out that this data-driven approach helps, as I now have better working definitions of the seasons and can judge my expectations accordingly.\nBefore we get to those conclusions, I take two steps with the data.\nFirst, I get it and clean it. Specifically, for each day of the year, I collect the maximum temperature achieved at Heathrow in each year from 1990 to 2022. I then average these 33 data points to get one value for the typical maximum temperature on that day at Heathrow.\n\n\n&lt;/&gt;\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(lubridate)\nlibrary(riem)\n\n# riem_stations(network = \"GB__ASOS\")\n\n# data_temp &lt;- riem_measures(\n#   station = \"EGLL\",\n#   date_start = \"1990-01-01\",\n#   date_end = \"2023-12-31\"\n#   )\n\n# write_rds(data_temp, \"data_temp.rds\")\n\ndata_temp &lt;- read_rds(\"data_temp.rds\")\n\ndata &lt;- data_temp |&gt; \n  select(valid, tmpf) |&gt; \n  mutate(\n    value_year = year(valid),\n    value_week = if_else(week(valid) == 53, 52, week(valid)),\n    value_month = month(valid),\n    value_day = day(valid),\n    value_hour = hour(valid),\n    value_minute = minute(valid),\n    value_dayofweek = wday(valid),\n    value_dayofyear = yday(valid),\n    value_fortnight = ceiling(value_week / 2)\n  ) |&gt; \n  summarise(\n    temp_max = max(tmpf, na.rm = TRUE),\n    .by = c(value_year, value_month, value_day)\n    ) |&gt; \n  summarise(\n    temp_max_ave = mean(temp_max, na.rm = TRUE),\n    .by = c(value_month, value_day)\n    ) |&gt; \n  rownames_to_column(var = \"value_daynumber\") |&gt; \n  mutate(\n    value_daynumber = as.integer(value_daynumber),\n    name_month = as_factor(month.name[value_month]),\n    value_date = make_date(year = 2019, month = value_month, day = value_day)\n    ) |&gt; \n  filter(!(value_month == 2 & value_day == 29))\n\nglimpse(data, width = 80)\n\n\nRows: 365\nColumns: 6\n$ value_daynumber &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,‚Ä¶\n$ value_month     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ value_day       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,‚Ä¶\n$ temp_max_ave    &lt;dbl&gt; 47.56235, 46.76294, 46.39176, 46.17941, 47.32471, 47.6‚Ä¶\n$ name_month      &lt;fct&gt; January, January, January, January, January, January, ‚Ä¶\n$ value_date      &lt;date&gt; 2019-01-01, 2019-01-02, 2019-01-03, 2019-01-04, 2019-‚Ä¶\n\n\n\nSecond, I‚Äôll chart how this average temperature varies by the day of the year.\n\n\n&lt;/&gt;\nplot_seasons &lt;- data |&gt; \n  ggplot(\n    aes(\n      x = value_daynumber,\n      y = temp_max_ave,\n      colour = name_month\n      )\n    ) + \n  scale_y_continuous(\n    labels = 40 + 0:3 * 10, \n    breaks = 40 + 0:3 * 10, \n    minor_breaks = NULL\n    ) +\n  scale_x_continuous(breaks = 30 * 0:12) + \n  scale_color_brewer(type = \"qual\", palette = 3) +\n  geom_point() + \n  theme_minimal() + \n  theme(\n    plot.title.position = 'plot',\n    plot.subtitle = element_text(size = 10),\n    axis.title.x = element_text(size = 10, hjust = 1),\n    legend.title = element_blank(),\n    panel.grid = element_blank(),\n    axis.line = element_line(colour = \"grey50\"),\n    axis.ticks = element_line(colour = \"grey50\")\n    ) + \n  labs(\n    title = \"Typical Maximum Daily Temperature at Heathrow\",\n    subtitle = \"Daily averages in Fahrenheit from 1990 to 2022\\n\",\n    y = NULL,\n    x = \"\\nDay of the year\"\n  ) \n\nplot_seasons\n\n\n\n\n\n\n\n\n\n\nOK, so that chart helps, but it is more useful if we add some context to it.\n\n\n&lt;/&gt;\nplot_seasons + \n  geom_spline(\n    aes(\n      x = value_daynumber,\n      y = temp_max_ave\n      ),\n    spar = 0.66,\n    colour = 'magenta', \n    linewidth = 1 \n    ) +\n  geom_abline(slope = 0.19, intercept = 38.5, color = 'grey15') + \n  geom_abline(slope = -0.25, intercept = 132.5, color = 'grey15') + \n  geom_vline(xintercept = 60, color = 'grey50', linetype = 3) +\n  geom_vline(xintercept = 165, color = 'grey50', linetype = 3) +\n  geom_vline(xintercept = 250, color = 'grey50', linetype = 3) +\n  geom_vline(xintercept = 330, color = 'grey50', linetype = 3) + \n  geom_hline(yintercept = 50, color = 'grey50', linetype = 3) + \n  geom_hline(yintercept = 70, color = 'grey50', linetype = 3)\n\n\n\n\n\n\n\n\n\n\nSo, what does this mean?\nBased upon the chart above, I will now use the following definitions:\n\nLong Linear Spring warms linearly and is the longest season, running from March until mid June\nShort Summer is the shortest season ‚Ä¶ üôÑ ‚Ä¶ lasting from mid June until the end of August, when typical daily temperatures exceed 70¬∞F\nLinear Autumn cools linearly over September, October and November\nWinter consists of December, January and February, when typical daily temperatures fall below 50¬∞F\n\nSo, daily temperature change in Spring and Autumn is typically linear. Sadly, though, Autumn cools faster than Spring warms. Specifically, Spring sees maximum temperatures typically rise by 0.19¬∞F per day, whilst the corresponding decline in Autumn is 0.25¬∞F per day. In other words, five days of gains in Spring are needed to counteract four days of losses in Autumn."
  },
  {
    "objectID": "posts/batting/index.html",
    "href": "posts/batting/index.html",
    "title": "Have batting averages improved over time?",
    "section": "",
    "text": "Batting averages seem to have changed over time, but not in the direction that you might expect.\nTypical averages have worsened over time, but only for top-order batsmen. Of course, that doesn‚Äôt necessarily mean that batting skill levels have declined, as other factors might also be involved.\nI‚Äôll now explain how I arrived at this conclusion and the assumptions that I made along the way. The starting point, as ever, was data. Specifically, the wonderful data from the espnCricinfo stats engine.\n\nlibrary(rvest)\nlibrary(tidyverse)\n\nscrape_stats &lt;- function(value_count){\n  value_url &lt;- paste0(\n    \"https://stats.espncricinfo.com/ci/engine/stats/index.html?batting_positionmax1=\",\n    value_count,\n    \";batting_positionmin1=\",\n    value_count,\n    \";batting_positionval1=batting_position;class=1;filter=advanced;opposition=1;opposition=2;opposition=3;opposition=4;opposition=5;opposition=6;opposition=7;opposition=8;orderby=runs;qualmin1=20;qualval1=innings;size=200;template=results;type=batting\"\n    )\n  \n  read_html(value_url) |&gt; \n    rvest::html_table() |&gt; \n    _[[3]]\n}\n\ndata_scraped &lt;- map(\n  .x = 1:11, \n  .f = ~scrape_stats(value_count = .x)\n  ) |&gt; \n  list_rbind(names_to = \"order_batting\")\n\nwrite_rds(x = data_scraped, file = \"data_scraped.rds\")\n\nThat said, I only consider:\n\nPlayers with twenty innings or more\nScores for and against the ‚Äòtop sides‚Äô (i.e.¬†Australia, England, India, New Zealand, Pakistan, South Africa, Sri Lanka and the West Indies) at some point\n\nAfter a little more tidying, we get the following data on about 1,000 batsmen, sorted by batting order and (descending) average.\n\nsuppressPackageStartupMessages(library(tidyverse))\n\nWarning: package 'lubridate' was built under R version 4.4.1\n\nlibrary(gganimate)\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.1\n\nlibrary(magick)\n\nWarning: package 'magick' was built under R version 4.4.1\n\ndata_batsmen &lt;- read_rds(\"data_scraped.rds\") |&gt; \n  mutate(\n    Name = word(Player, start = 1L, end = -2L),\n    fullCountry = word(Player, -1),\n    Country = str_sub(fullCountry, 2,-2),\n    Start = as.integer(str_sub(Span, 1, 4)),\n    Decade = 10*trunc(Start/10),\n    Name = str_replace_all(Name, \"'\", \" \") \n    ) |&gt; \n  filter(!(Country %in% c(\"BAN\", \"ZIM\"))) |&gt; \n  select(Name, Country, Decade, \"Average\" = Ave, \"Innings\" = Inns, \"Order\" = order_batting) |&gt; \n  arrange(Order, desc(Average))\n\ndata_batsmen\n\n\n  \n\n\n\n\nWe can then analyse the data by batting order, using the splendid gganimate.\n\ndata_batsmen |&gt; \n  ggplot(\n    aes(\n      x = Decade,\n      y = Average,\n      color = Country, \n      size = Innings\n      )\n    ) +\n  geom_point(alpha = 1) +\n  labs(\n    x = \"First decade of the batsman's career\",\n    y = \"\"\n    ) + \n  theme_minimal() +\n  ggtitle(\n    'Players who have ever batted at {closest_state} in the order',\n    subtitle = 'Average when batting at that position'\n    ) + \n  transition_states(\n    states = Order,\n    transition_length = 2,\n    state_length = 1\n    ) + \n  ease_aes('cubic-in-out') \n\n\n\n\n\n\n\n\n\nAlthough that looks cool, I find it hard to detect patterns in animations, and so switch to a static plot.\n\ndata_batsmen |&gt; \n  ggplot(\n    aes(\n      x = Order,\n      y = Average,\n      colour = Decade\n      )\n    ) +\n  geom_jitter() +\n  scale_x_continuous(labels = c(1, 3, 5, 7, 9, 11), breaks = c(1, 3, 5, 7, 9, 11)) +\n  scale_y_continuous(labels = c(0, 50, 100), breaks = c(0, 50, 100)) +\n  facet_wrap(~Decade) +\n  theme_minimal() +\n  labs(\n    title = 'Data is limited before the 1950s\\n',\n    subtitle = 'Batting average, when batting at that position, for a career that started in the given decade',\n    x = \"\\nBatting order\",\n    y = NULL\n    ) + \n  theme(\n    plot.title.position = \"plot\",\n    legend.position = \"none\",\n    axis.title.x = element_text(hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\nGiven the paucity of data before the 1950s and in this decade, it makes sense to drop these decades from our analysis. If we do so and also add best-fit lines to the decade-by-decade charts above, we get the following.\n\ndata_batsmen |&gt; \n  filter(\n    between(Decade, 1950, 2010)\n  ) |&gt;\n  mutate(Decade = as_factor(Decade)) |&gt; \n  ggplot(\n    aes(\n      x = Order,\n      y = Average,\n      colour = Decade\n      )\n    ) +\n  geom_jitter() + \n  geom_smooth(se = FALSE) + \n  scale_colour_brewer(palette = \"Blues\", direction = -1) +\n  scale_x_continuous(labels = c(1, 3, 5, 7, 9, 11), breaks = c(1, 3, 5, 7, 9, 11)) +\n  scale_y_continuous(labels = c(0, 50, 100), breaks = c(0, 50, 100)) +\n  facet_wrap(~Decade) +\n  theme_minimal() +\n  labs(\n    title = 'Typical averages by batting order follow a similar shape over the decades\\n',\n    subtitle = 'Batting average, when batting at that position, for a career that started in the given decade',\n    x = \"\\nBatting order\",\n    y = NULL\n    ) + \n  theme(\n    plot.title.position = \"plot\",\n    legend.position = \"none\",\n    axis.title.x = element_text(hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\nTo get a better sense of the trends, let‚Äôs now just consider these best-fit lines as one chart, where darker lines represent earlier starting decades of a career.\n\ndata_batsmen |&gt; \n  filter(\n    between(Decade, 1950, 2010)\n  ) |&gt; \n  mutate(Decade = as_factor(Decade)) |&gt; \n  ggplot(\n    aes(\n      x = Order,\n      y = Average,\n      colour = Decade,\n      group = Decade\n      )\n    ) +\n  geom_smooth(se = FALSE) + \n  scale_colour_brewer(palette = \"Blues\", direction = -1) +\n  scale_x_continuous(labels = 1:11, breaks = 1:11, minor_breaks = NULL) +\n  scale_y_continuous(minor_breaks = NULL, limits = c(0, NA), labels = 20 * 0:2, breaks = 20 * 0:2) +\n  theme_minimal() +\n  labs(\n    title = 'Higher-order averages have typically declined since the 1950s\\n',\n    subtitle = 'Typical batting average, when batting at that position, for a career that started in the given decade',\n    colour = \"Decade start\",\n    x = \"\\nBatting order\",\n    y = NULL\n    ) + \n  theme(\n    plot.title.position = \"plot\",\n    axis.title.x = element_text(hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\nSo, what does this chart show us?\nTo me, it shows that typical latter-order averages haven‚Äôt changed that much over the decades, whilst there‚Äôs a lot more variation by decade for the typical averages of higher-order batsmen. Furthermore, the lines with higher typical top-order averages all seem to be darker (and therefore earlier).\nTo get more specific, let‚Äôs split this top-order data into two eras: the 1950s to the 1970s; and the 1980s and beyond. I can then compare the typical top-order batting averages in these eras.\n\ndata_test &lt;- data_batsmen |&gt; \n  filter(\n    between(Decade, 1950, 2010),\n    Order &lt;= 5\n  ) |&gt; \n  mutate(is_1950s_to_1970s = Decade &lt;= 1970) |&gt; \n  select(Average, is_1950s_to_1970s) \n\nt.test(\n  data = data_test, \n  Average ~ is_1950s_to_1970s, \n  alternative = \"less\"\n  )\n\n\n    Welch Two Sample t-test\n\ndata:  Average by is_1950s_to_1970s\nt = -2.0788, df = 348.5, p-value = 0.01918\nalternative hypothesis: true difference in means between group FALSE and group TRUE is less than 0\n95 percent confidence interval:\n       -Inf -0.4100609\nsample estimates:\nmean in group FALSE  mean in group TRUE \n           39.55741            41.54190 \n\n\n\nOK, so that‚Äôs a lot of detail! But what does it show?\nEssentially, it says that the reduction in typical top-order batting averages since the 1950s to 1980s is statistically significant. (That is, beyond what we would usually expect from chance alone.)\nOf course, this reduction might not be solely due to lower inherent skill levels relative to the prevailing bowlers. Two possible reasons are that:\n\nThe game is now different to before, with DRS, helmets and masses of analysis\nSurvivor bias might be a factor. After all, we are only considering players with twenty or more innings, for and against the ‚Äòtop sides‚Äô; a point also complicated by the fact that there are now more test matches per year than in the past\n\nAs is often the way with cricket, so much is in the interpretation and debate. And that‚Äôs just another reason to love it!\n\n\n\n[Updated on March 25, 2023]"
  },
  {
    "objectID": "posts/portfolioRisk/index.html",
    "href": "posts/portfolioRisk/index.html",
    "title": "Projecting portfolio risk",
    "section": "",
    "text": "To do so, I‚Äôll deliberately pick a minimal example and assume that our portfolio has the thirty-stock Dow-Jones Industrial Average (DJIA) as its index.\nI‚Äôll show the results in a couple of paragraphs. Before that, let‚Äôs consider a little of the theory behind this analysis. Specifically, that the active variance of a portfolio relative to its index is: \\[\\omega^2 = X^T V X\\]\n, where:\nGiven this equation, I see that I need to begin the analysis by downloading the holdings of the index and the pricing data for the constituent stocks.\nlibrary(tidyquant)\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(waldo)\n\n# weights_index &lt;- tq_index(\"DOW\")\n# write_rds(x = weights_index, file = \"weights_index.rds\")\n\n# stock_prices  &lt;- tq_get(\n#   x = weights_index |&gt; pull(symbol), \n#   get = \"stock.prices\", \n#   from = \"2022-01-01\")\n# write_rds(x = stock_prices, file = \"stock_prices.rds\", compress = \"xz\")"
  },
  {
    "objectID": "posts/portfolioRisk/index.html#footnotes",
    "href": "posts/portfolioRisk/index.html#footnotes",
    "title": "Projecting portfolio risk",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor the sake of simplicity, I only consider split-adjusted price returns rather than the equivalent total returns in this example‚Ü©Ô∏é\nThis might be a heroic assumption, as it implies an independence between the risks experienced in different months‚Ü©Ô∏é\nNote that the sum of these values will add to the total active variance, as this calculation is essentially vector multiplication.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/brexit/index.html",
    "href": "posts/brexit/index.html",
    "title": "Brexit vote analysis",
    "section": "",
    "text": "You can scroll down for an interactive chart of each constituency, but the following chart shows the main detail.\n\n\n&lt;/&gt;\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(ggiraph)\nlibrary(glue)\nlibrary(ggplot2)\nlibrary(ggtext)\nlibrary(parlitools)\nlibrary(scales)\n\nmap_details &lt;- west_hex_map\nsf::st_crs(map_details) = 4326\n\ndata_brexit &lt;- leave_votes_west |&gt;  \n  rename(`Leave Vote` = figure_to_use) |&gt; \n  mutate(\n    `Party of MP` = as_factor(party_2016), \n    `Party of MP` = \n      recode(\n        `Party of MP`,\n        `Scottish National Party` = \"SNP\", \n        `Liberal Democrat` = \"LibDem\"\n        ),\n    `Party of MP` = fct_lump(\n      `Party of MP`,\n      n = 5, \n      other_level = \"Other\"\n      )\n    ) |&gt; \n  left_join(\n    map_details,\n    by = c(\"ons_const_id\" = \"gss_code\")\n    ) |&gt; \n  mutate(\n    constituency = if_else(\n      is.na(constituency_name.x), \n      constituency_name.y, \n      constituency_name.x\n      ),\n    vote_status = case_when(\n      (`Leave Vote` &gt;= 0) & (`Leave Vote` &lt;= 0.4) ~ \"Remain\",\n      (`Leave Vote` &gt; 0.4) & (`Leave Vote` &lt;= 0.6) ~ \"Close Call\",\n      (`Leave Vote` &gt; 0.6) & (`Leave Vote` &lt;= 1) ~ \"Leave\",\n      TRUE ~ \"Error\"\n      ),\n    vote_status = as_factor(vote_status),\n    vote_status = fct_relevel(vote_status, sort)\n    ) |&gt; \n  select(constituency, `Leave Vote`, `Party of MP`, ons_const_id, vote_status, geometry)\n\nhtml_text &lt;- glue(\"&lt;span&gt;Showing constituencies with &lt;span style='color:#0087DC;'&gt;Conservative&lt;/span&gt;, &lt;span style='color:#DC241F;'&gt;Labour&lt;/span&gt;, &lt;span style='color:#FFFF00;'&gt;SNP&lt;/span&gt;, &lt;span style='color:#FDBB30;'&gt;LibDem&lt;/span&gt; and &lt;span style='color:#AFAFAFAF;'&gt;Other&lt;/span&gt; MPs&lt;/span&gt;&lt;br/&gt;\")\n\ndata_brexit |&gt; \n  ggplot() + \n  geom_sf(\n    aes(\n      group = constituency,\n      geometry = geometry,\n      fill = `Party of MP`\n      ),\n    size = 0.1\n    ) + \n  scale_fill_manual(\n    values = c(\n      Conservative = \"#0087DC\",\n      Labour = \"#DC241F\",\n      SNP = \"#FFFF00\",\n      LibDem = \"#FDBB30\",\n      Other = \"grey80\"\n      )\n    ) + \n  facet_grid(~vote_status, margins = TRUE, drop = TRUE) +\n  theme_void() + \n  labs(\n    title = \"Leave voters seem to have a bigger split in party allegiance\",\n    subtitle = html_text\n  ) + \n  theme(\n    plot.title.position = \"plot\",\n    plot.subtitle = element_markdown(),\n    legend.position = \"none\",\n    plot.background = element_rect(\n      fill = \"grey90\", \n      colour = \"grey90\", \n      linewidth = 1\n      ),\n    plot.margin = margin(0.5, 0.5, 0.5, 0.5, \"cm\")\n    )\n\n\n\n\n\n\n\n\n\nBasically, outside of Scotland, there seems to be a bigger split in Leaver constituencies than in their Remainer counterparts.\nTo get the details for each constituency, hover over the relevant spot in the chart below.\n\n\n&lt;/&gt;\ngg &lt;- data_brexit |&gt; \n  ggplot() + \n  geom_sf_interactive(\n    aes(\n      group = constituency,\n      geometry = geometry,\n      fill = `Leave Vote`,\n      colour = `Party of MP`,\n      tooltip = paste0(\n        constituency, \n        \"\\n MP: \",\n        `Party of MP`,\n        \"\\n Leave vote: \",\n        round(`Leave Vote` * 100, 0),\n        \"%\"\n        )\n    ),\n    size = 0.1\n  ) + \n  scale_fill_gradient_interactive(\n    low = \"white\", \n    high = \"#63666a\",\n    labels = percent_format(accuracy = 1)\n    ) +\n  scale_colour_manual_interactive(\n    values = c(\n      Conservative = \"#0087DC\",\n      Labour = \"#DC241F\",\n      SNP = \"#FFFF00\",\n      LibDem = \"#FDBB30\",\n      Other = \"grey80\"\n      )\n    ) + \n  theme_void() +\n  labs(\n    title = \"Leave vote and winning party, by constituency\", \n    subtitle = \"Hover over a point for the details\"\n  ) + \n  theme(\n    legend.position = \"right\",\n    legend.text = element_text(size = 6, hjust = 0),\n    legend.title = element_text(size = 8)\n    )\n\ngirafe(ggobj = gg)"
  },
  {
    "objectID": "posts/snippets/index.html",
    "href": "posts/snippets/index.html",
    "title": "Code that I often use but rarely recall",
    "section": "",
    "text": "Specifically, these snippets enable me to:\n\nCustomise quarto output\nGenerate ggplot2 charts in the style that I use\nFormat tables in my preferred way\nAccess databases inside and outside of shiny\nSolve common niggles in package building\n\nTo demonstrate these code snippets, I‚Äôll use the tidyverse packages with data from palmerpenguins.\n\nsuppressPackageStartupMessages(library(tidyverse))\nlibrary(palmerpenguins)\n\nNote: In each case below, you can copy the code by hovering over it and selecting the clipboard icon in the top-right of the code chunk.\n\n\n1. Customise quarto output\nGiven my frequent use of Quarto notebooks, I‚Äôve gravitated to these settings that work best for me.\nHowever, you can easily tweak these settings to suit you, as each option has an accompanying ‚Äòtab auto-complete‚Äô feature.\n---\ntitle: \"Add title here\"\nsubtitle: \"Add subtitle here\"\nauthor:\n  - Robin Penfold \ndate: today\nformat: \n  html:\n    anchor-sections: true \n    code-copy: hover \n    code-fold: true \n    code-link: true \n    code-overflow: wrap \n    code-summary: \"&lt;/&gt;\" \n    code-tools: false \n    df-print: paged \n    embed-resources: true \n    float: true\n    footnotes-hover: true \n    highlight-style: pygments\n    lang: en-GB \n    linkcolor: \"#63431c\"\n    mainfont: \"Arial\"\n    table-of-contents: true \n    toc-depth: 4\n    toc-title: \" \"\n    title-block-banner: \"#edd9c0\"\n    title-block-banner-color: \"#63431c\"\n    title-block-categories: false\neditor_options:\n  chunk_output_type: inline\n---\n\n\n\n2. Generate ggplot2 charts in the style that I use\nOver time, I have coalesced towards the following small chunk of code that builds (what I consider to be) a decent-looking chart in ggplot2.\n\npenguins |&gt; \n  ggplot(\n    aes(\n      x = bill_length_mm,\n      y = body_mass_g\n      )\n    ) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\", \n    se = FALSE, \n    colour = \"#63431c\"\n    ) +\n  labs(\n    title = \"Penguins with longer bills tend to be heavier\\n\",\n    subtitle = \"Body mass (g)\",\n    x = \"\\nBill length (mm)\",\n    y = NULL\n    ) +\n  scale_y_continuous(labels = scales::label_comma()) +\n  theme_minimal() +\n  theme(\n    plot.title.position = \"plot\",\n    plot.title = element_text(size = 14, colour = \"#63431c\"),\n    axis.title.x = element_text(hjust = 1)\n    )\n\n\n\n\n\n\n\n\n\n\n\n3. Format tables in my preferred way\nI occasionally print tables using the out-of-the-box settings (admittedly tweaked by using the df-print: paged option in part 1 above). This generates a table as follows.\n\npenguins\n\n\n  \n\n\n\nOtherwise, I usually use reactable with the following tweaks (the results of which I omit for brevity) ‚Ä¶\n\n# library(reactable)\n\npenguins |&gt; \n  select(species, island, bill_length_mm, bill_depth_mm, body_mass_g) |&gt; \n  reactable(\n    filterable = TRUE, \n    highlight = TRUE, \n    borderless = TRUE, \n    defaultPageSize = 5, \n    columns = list(\n      species = colDef(name = \"Species\", minWidth = 90, sticky = \"left\"),\n      island = colDef(name = \"Island\", minWidth = 90, sticky = \"left\"),\n      bill_length_mm = colDef(name = \"Bill length\", sticky = \"right\", filterable = FALSE, format = colFormat(separators = TRUE, digits = 1)),\n      bill_depth_mm = colDef(name = \"Bill depth\", sticky = \"right\", filterable = FALSE, format = colFormat(separators = TRUE, digits = 1)),\n      body_mass_g = colDef(name = \"Body mass\", sticky = \"right\", filterable = FALSE, format = colFormat(separators = TRUE, digits = 0))\n      )\n    )\n\n‚Ä¶ or I use DT (i.e.¬†the datatable package).\nlibrary(DT)\n\npenguins |&gt; \n  select(\"Species\" = species, \"Island\" = island, \"Bill length\" = bill_length_mm, \"Bill depth\" = bill_depth_mm, \"Body mass\" = body_mass_g) |&gt; \n  datatable(\n    rownames = FALSE, \n    width = \"100%\",\n    options=list(\n      dom = 'tip',\n      pageLength = 5\n      )\n    ) |&gt; \n  formatRound(\n    columns = 3:4,\n    digits = 1\n    ) |&gt; \n  formatRound(\n    columns = 5,\n    digits = 0\n    )\n\n\n\n\n\n4. Access databases inside and outside of shiny\nI use the wonderful DBI and dbplyr all the time, not least for exploratory analysis.\n(Note that whilst I typically don‚Äôt use SQLite, I will do so here, as it plays better with my website architecture.)\n\nlibrary(RSQLite)\n\ncon &lt;- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"instruments\", dplyr::band_instruments)\ndbWriteTable(con, \"members\", dplyr::band_members)\n\ndbListTables(con)\n\n[1] \"instruments\" \"members\"    \n\n\nIn this example, we create an object (con) for connecting to the SQLite database, where we add two tiny tables, called instruments and members. We can then explore these tables.\n\ntbl(src = con, \"instruments\")\n\n\n  \n\n\n\n\ntbl(src = con, \"members\")\n\n\n  \n\n\n\nEven better, we can explore the tables when they are combined and tidied. (Whilst the code appears to return all the data, that‚Äôs only because our tables are uncommonly small.)\n\ntbl(src = con, \"instruments\") |&gt; \n  left_join(\n    tbl(src = con, \"members\"), \n    by = \"name\"\n  ) |&gt; \n  filter(band == \"Beatles\")\n\n\n  \n\n\n\nOnce you have what you need, assign a name to the code and append it with |&gt; collect().\n\nWhilst this functionality is great outside of shiny, it is often more valuable within it. (After all, these apps can be a really safe and simple way for users to access a corporate database.)\nTo do so, some other tweaks are required within shiny‚Äôs server functionality, as illustrated below.\n\ndata_chosen &lt;- shiny::reactive({\n  shiny::req(input$dataset)\n  main_data |&gt; \n    dplyr::filter(name_dataset == input$dataset) |&gt; \n    dplyr::mutate(id = as.integer(id))\n  })\n\ndata_chosen_id &lt;- shiny::reactive(\n  quote({data_chosen()$id}),\n  quoted = TRUE\n  )\n\ndata_calculated &lt;- shiny::reactive({\n  arbitrary_function(\n    con,\n    arbitrary_argument = data_chosen_id()\n    )\n  })\n\n\n\n\n5. Solve common niggles in package building\nWhen I‚Äôm building packages, I often get dinged with notes or warnings about two common package niggles.\nThe first of these is non-ASCII characters. With apologies for the person who showed me, and who I now can‚Äôt recall, you can find these characters by:\n\nClicking CTRL + F in RStudio\nSelecting the Regex tick-box\nEntering: [\\u0080-\\uFFFF] or [^\\x00-\\x7F] as the search term\n\nFrom there, you can use stringi::stri_escape_unicode('@') to get the Unicode equivalent for @. (Note that you also might need to remove the initial ‚Äò\\‚Äô on Windows.)\n\nThe second niggle of package building concerns variable binding. It occurs during the package check and creates a note along the following lines.\nno visible binding for global variable\n    ‚ÄòABC‚Äô\nIn this case, we need to do something of the form below (from my simple package p0bservations that you can find here).\n\n#' @title Calculate income net of UK tax and National Insurance\n#'\n#' @description This function ...\n#' @param income_taxable The taxable income level ...\n#' @param tax_year_end The calendar year in which the tax year ends ...\n#' @export\n#' @examples\n#' \\dontrun{\n#' calc_income_net(income_taxable = 38000, tax_year_end = 2022L)\n#' }\n#' \n#' @importFrom rlang .data\n\nOnce we have added the line of #' @importFrom rlang .data, we can call the variables as follows (i.e.¬†as before, but preceded by .data$).\n\nyear_tax_end_options &lt;- p0bservations::tax_parameters |&gt; \n  dplyr::distinct(.data$year_tax_end) |&gt; \n  dplyr::pull(.data$year_tax_end)\n\n\nOnce again, I hope that this aide-m√©moire helps you as well as me!"
  },
  {
    "objectID": "posts/wordle/index.html",
    "href": "posts/wordle/index.html",
    "title": "Winning at Wordle",
    "section": "",
    "text": "Let‚Äôs start by getting the ~500 most commonly-occurring five-letter words (that I downloaded as a csv file from the internet).\n\nsuppressPackageStartupMessages(library(tidyverse))\n\nwords &lt;- \n  read_csv(\n    file = \"five-letters.csv\", \n    col_names = FALSE\n    ) |&gt; \n  rename(\"word\" = X1) |&gt; \n  mutate(word = str_to_lower(word)) |&gt; \n  mutate(\n    l1 = str_sub(string = word, start = 1, end = 1),\n    l2 = str_sub(string = word, start = 2, end = 2),\n    l3 = str_sub(string = word, start = 3, end = 3),\n    l4 = str_sub(string = word, start = 4, end = 4),\n    l5 = str_sub(string = word, start = 5, end = 5)\n  )\n\nwords\n\n\n  \n\n\n\nFor reference, I‚Äôll also chart the popularity of each letter by their order in these five-letter words.\n\nwords_long &lt;- words |&gt; \n  pivot_longer(\n    cols = -word, \n    names_to = \"measure\", \n    values_to = \"values\"\n    ) |&gt; \n  mutate(\n    position = as.integer(\n      str_sub(string = measure, start = 2)\n      )\n    ) |&gt; \n  select(values, position)\n\nwords_long |&gt; \n  count(values, position) |&gt; \n  mutate(\n    position = case_match(\n      position,\n      1 ~ \"1st\",\n      2 ~ \"2nd\",\n      3 ~ \"3rd\",\n      4 ~ \"4th\",\n      5 ~ \"5th\"\n      )\n    ) |&gt; \n  ggplot(\n    aes(\n      x = values, \n      y = n, \n      fill = values %in% c(\"a\", \"e\", \"r\", \"s\", \"t\")\n      )\n    ) + \n  geom_col() + \n  scale_y_continuous(\n    limits = c(0, NA), \n    minor_breaks = NULL, \n    expand = expansion(mult = 0, add = 1)\n    ) + \n  scale_fill_manual(values = c(\"#edd9c0\", \"#63431c\")) +\n  facet_wrap(~position, nrow = 1) + \n  theme_minimal() + \n  labs(\n    title = \"Frequency of letter by word order\",\n    subtitle = \"Emphasis on the letters a, e, r, s and t\\n\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.title.position = \"plot\"\n    )\n\n\n\n\n\n\n\n\nAs you can see from the emphasis, some of these letters appear a lot more than others, and especially at the start and end of the word."
  },
  {
    "objectID": "posts/wordle/index.html#footnotes",
    "href": "posts/wordle/index.html#footnotes",
    "title": "Winning at Wordle",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese ‚Äòbest‚Äô guesses might not be perfect, as my assumptions above and the approach in general could probably be improved, perhaps with Operational Research techniques. That said, I suspect that ‚Äòstare‚Äô and the next guesses are decent approximations to the ideal solution.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/evaluation/index.html",
    "href": "posts/evaluation/index.html",
    "title": "Asset manager evaluation",
    "section": "",
    "text": "The idea is pretty simple, which I‚Äôll outline below. (I can do so, as the materials are already in the public domain.)\nIf you want the details, check out the formal paper at SSRN. For a more accessible introduction, I‚Äôd recommend the video below that was taken (and won an award) at the International Congress of Actuaries in 2014.\n\n\n\nThe gist of the idea\nIn short, a key mantra of investment advisors is that asset owners should ignore the performance of their asset manager. Whether the manager has outperformed for you or not, it is claimed, is irrelevant to how you should expect them to perform in the future. However, a key idea in statistics, called Bayesian thinking, suggests the opposite: that you should use any new information you get about something to update your thinking on it.\nIn this work, I therefore used Bayesian thinking to understand the advice that asset owners should get. It turns out that this advice makes common sense, particularly when paired with an understanding of whether the asset manager has some longer-term cyclicality in their performance.\nHere‚Äôs that video that I mentioned:"
  },
  {
    "objectID": "posts/prempredict/index.html",
    "href": "posts/prempredict/index.html",
    "title": "PremPredict",
    "section": "",
    "text": "After a good run of thirteen years, now feels like the right time to end our PremPredict competition.\nThanks to all of you who have shared in the fun over this time. I‚Äôm pleased to say that this included Les Penfold, Mike Finnis and Roger Gathercole.\nAs you‚Äôll see below, though, this post and the Wall of Fame will remain here, always showing that Peter Finnis was left holding the crown!\n(Oh yeah ‚Äì and always showing how some of the biggest football nerds remained winless.)\n\n\n\nPremPredict champions\n\n2007/08 ‚Äì Robert Wye\n2008/09 ‚Äì Les Penfold\n2009/10 ‚Äì Beth Penfold\n2010/11 ‚Äì George Quin and Miranda Stride\n2011/12 ‚Äì Hannah Finnis\n2012/13 ‚Äì Michael Cheeseman\n2013/14 ‚Äì Danny Russell\n2014/15 ‚Äì Mathew Saunders\n2015/16 ‚Äì Roger Gathercole\n2016/17 ‚Äì Luke Finnis\n2017/18 ‚Äì George Quin\n2018/19 ‚Äì Roger Gathercole\n2019/20 ‚Äì Peter Finnis\n\n\nThanks again for all the fun!"
  }
]